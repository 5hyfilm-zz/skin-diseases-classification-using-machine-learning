{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nadam_Focal-Loss_DenseNet121_CAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM4A23LP+hPU+QmXMdn7xAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filmerxyz/JSTP-22_SkinDiseaseClassificationUsingMachineLearning/blob/master/Nadam_Focal_Loss_DenseNet121_CAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGAbDjt3rz_5",
        "colab_type": "text"
      },
      "source": [
        "# **Check GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KeAnM3PXlUe",
        "colab_type": "code",
        "outputId": "c58799e8-f75b-41c3-a828-d18ee9fd6924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 28 08:07:14 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTSt9Wpjb1bl",
        "colab_type": "code",
        "outputId": "1f3e92d0-12c1-4650-c77d-c7c78147c377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhDHLrLxbkCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install keras-lookahead\n",
        "\n",
        "# !pip install keras-rectified-adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbD3Qq816yZZ",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tch5If72HQeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "from ham10000_utils_functions import plot_confusion_matrix, normalize, deprocess_image, my_decode_predictions, guided_backprop, grad_cam, compute_saliency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPwDTos761Oq",
        "colab_type": "text"
      },
      "source": [
        "# **Clone Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjfNRvqza-U",
        "colab_type": "code",
        "outputId": "da41ed17-3ded-4e95-c293-b32f450eef54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/EvilPickle-PCSHSPT/ham10000-with-one-image-folder"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ham10000-with-one-image-folder'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10045 (delta 4), reused 7 (delta 2), pack-reused 10036\n",
            "Receiving objects: 100% (10045/10045), 2.57 GiB | 15.75 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Checking out files: 100% (10022/10022), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__IXKxBZVymL",
        "colab_type": "text"
      },
      "source": [
        "# **Constant Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ2LKSeHQRnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "LR = 3e-5 # Learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrpBmL7mVlJC",
        "colab_type": "text"
      },
      "source": [
        "# **Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnsBJBcPL3SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('/content/ham10000-with-one-image-folder/HAM10000_metadata.csv')\n",
        "data['image_full_name']=data['image_id']+'.jpg'\n",
        "X=data[['image_full_name','dx','lesion_id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rt4v6YTM0Fr",
        "colab_type": "code",
        "outputId": "f7fb4338-885e-4d81-9007-9d7cdd06d126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>image_full_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0027419</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0027419.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0025030</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0025030.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0026769</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0026769.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0025661</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0025661.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0001466</td>\n",
              "      <td>ISIC_0031633</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>75.0</td>\n",
              "      <td>male</td>\n",
              "      <td>ear</td>\n",
              "      <td>ISIC_0031633.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id      image_id   dx  ...   sex  localization   image_full_name\n",
              "0  HAM_0000118  ISIC_0027419  bkl  ...  male         scalp  ISIC_0027419.jpg\n",
              "1  HAM_0000118  ISIC_0025030  bkl  ...  male         scalp  ISIC_0025030.jpg\n",
              "2  HAM_0002730  ISIC_0026769  bkl  ...  male         scalp  ISIC_0026769.jpg\n",
              "3  HAM_0002730  ISIC_0025661  bkl  ...  male         scalp  ISIC_0025661.jpg\n",
              "4  HAM_0001466  ISIC_0031633  bkl  ...  male           ear  ISIC_0031633.jpg\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_1JsUvHGMGi",
        "colab_type": "text"
      },
      "source": [
        "### **Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1q1WsAhM-HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y=X.pop('dx').to_frame()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ghodz0zOJ0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.concat([X_train,y_train],axis=1)\n",
        "val = pd.concat([X_val,y_val],axis=1)\n",
        "test = pd.concat([X_test,y_test],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG6iaiiyMHmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train['dx'])\n",
        "name_as_indexes_train = encoder.transform(train['dx']) \n",
        "train['label'] = name_as_indexes_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLryd9huOStO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(val['dx'])\n",
        "name_as_indexes_val = encoder.transform(val['dx']) \n",
        "val['label'] = name_as_indexes_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VYdnvBOOUek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder=LabelEncoder()\n",
        "encoder.fit(test['dx'])\n",
        "name_as_indexes_test = encoder.transform(test['dx']) \n",
        "test['label'] = name_as_indexes_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDOtGZx7YiJa",
        "colab_type": "text"
      },
      "source": [
        "### **Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6snNZRZOWaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = ImageDataGenerator(rescale = 1./255,\n",
        "                                     rotation_range=360,  \n",
        "                                     zoom_range = 0.3,\n",
        "                                     horizontal_flip=True,\n",
        "                                     vertical_flip=True,\n",
        "                                     fill_mode='reflect')\n",
        "                                    \n",
        "test_generator=ImageDataGenerator(rescale = 1./255)\n",
        "test_generator=ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdF3KvYwOfC5",
        "colab_type": "code",
        "outputId": "02bfe0d1-0e55-4bb6-bba9-f07d660f153a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data= train_generator.flow_from_dataframe(dataframe=train,x_col=\"image_full_name\",y_col=\"dx\",\n",
        "                                                directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                                shuffle=True,batch_size=32,class_mode=\"categorical\",target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "val_data= test_generator.flow_from_dataframe(dataframe=val,x_col=\"image_full_name\",y_col=\"dx\",\n",
        "                                              directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                              shuffle=True,batch_size=32,class_mode='categorical',target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "test_data= test_generator.flow_from_dataframe(dataframe=test,x_col=\"image_full_name\",y_col=\"dx\",\n",
        "                                              directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                              shuffle=False,batch_size=1,class_mode=None,target_size=(IMG_WIDTH,IMG_HEIGHT))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6309 validated image filenames belonging to 7 classes.\n",
            "Found 2704 validated image filenames belonging to 7 classes.\n",
            "Found 1002 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WOs0EjX8Ot2",
        "colab_type": "text"
      },
      "source": [
        "# **Focal Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9H8QLiwJ_rc",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/mkocabas/focal-loss-keras\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he4CnLjdH8c5",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHu6edXfTaAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\tdef focal_loss_fixed(y_true, y_pred):\n",
        "\t\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "\t\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\t\treturn -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\treturn focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivQqzr9X8T8Z",
        "colab_type": "text"
      },
      "source": [
        "# **Build Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6NKOM5-VHcm",
        "colab_type": "text"
      },
      "source": [
        "### **Use ResNet50 + fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982Ib3LtLZ19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  base_model = tf.keras.applications.ResNet50(include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), weights='imagenet')\n",
        "  \n",
        "  for layer in base_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "  average_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "  fc_layer = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(average_pooling_layer)\n",
        "  fc_layer = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(fc_layer)\n",
        "  fc_layer = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(fc_layer)\n",
        "  fc_layer = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(fc_layer)\n",
        "  bn_layer = tf.keras.layers.BatchNormalization()(fc_layer)\n",
        "  dropout_layer = tf.keras.layers.Dropout(0.25)(bn_layer)\n",
        "  prediction_layer = tf.keras.layers.Dense(units=NUM_CLASSES, activation='softmax', name='prediction')(dropout_layer)\n",
        "  model = tf.keras.models.Model(inputs=base_model.input, outputs=prediction_layer)\n",
        "  \n",
        "  model.compile(optimizer=Nadam(LR), loss=[focal_loss(alpha=.25, gamma=2)], metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quSBa9F3Qdqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TFNv-FGLdgd",
        "colab_type": "text"
      },
      "source": [
        "### **Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1bAKncT_-0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c2fd76f-9f05-4316-9165-148d494a0a2e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          524544      global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          32896       dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 64)           8256        dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 32)           2080        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32)           128         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32)           0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Dense)              (None, 7)            231         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,155,847\n",
            "Trainable params: 621,191\n",
            "Non-trainable params: 23,534,656\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUkpZH2iO2HT",
        "colab_type": "text"
      },
      "source": [
        "### **Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W01_QL-DQujb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/Nadam_ResNet50_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-TRuxDhSBVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1,\n",
        "                             save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPtUnf9tSCYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnTxHeV9ym7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_plateau = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=.5, min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGLVL4QTSDOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cb_list = [checkpoint, early_stop, reduce_plateau]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tWxLCM5VFYz",
        "colab_type": "text"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-tzC-r3SEBp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "484ab584-1f92-455b-83c5-775ea87531ab"
      },
      "source": [
        "history = model.fit_generator(generator=train_data,\n",
        "                            steps_per_epoch=train_data.samples//train_data.batch_size,\n",
        "                            validation_data=val_data,\n",
        "                            verbose=1,\n",
        "                            validation_steps=val_data.samples//val_data.batch_size,\n",
        "                            epochs=EPOCHS,\n",
        "                            callbacks=cb_list)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.1761Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.1955 - acc: 0.1112\n",
            "Epoch 00001: val_loss improved from -inf to 0.19555, saving model to /content/Nadam_ResNet50_model.h5\n",
            "197/197 [==============================] - 185s 940ms/step - loss: 0.1528 - acc: 0.1767 - val_loss: 0.1955 - val_acc: 0.1112\n",
            "Epoch 2/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.3103Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.3747 - acc: 0.1116\n",
            "Epoch 00002: val_loss improved from 0.19555 to 0.37473, saving model to /content/Nadam_ResNet50_model.h5\n",
            "197/197 [==============================] - 141s 718ms/step - loss: 0.1323 - acc: 0.3110 - val_loss: 0.3747 - val_acc: 0.1116\n",
            "Epoch 3/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.4152Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.3475 - acc: 0.1209\n",
            "Epoch 00003: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 141s 713ms/step - loss: 0.1247 - acc: 0.4153 - val_loss: 0.3475 - val_acc: 0.1209\n",
            "Epoch 4/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.4746Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.1858 - acc: 0.1756\n",
            "Epoch 00004: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 140s 713ms/step - loss: 0.1202 - acc: 0.4752 - val_loss: 0.1858 - val_acc: 0.1756\n",
            "Epoch 5/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.5267Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.1146 - acc: 0.5647\n",
            "Epoch 00005: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 140s 709ms/step - loss: 0.1165 - acc: 0.5267 - val_loss: 0.1146 - val_acc: 0.5647\n",
            "Epoch 6/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.5612Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.1058 - acc: 0.6536\n",
            "Epoch 00006: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 139s 707ms/step - loss: 0.1134 - acc: 0.5619 - val_loss: 0.1058 - val_acc: 0.6536\n",
            "Epoch 7/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.5880Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.1012 - acc: 0.6871\n",
            "Epoch 00007: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 139s 704ms/step - loss: 0.1108 - acc: 0.5880 - val_loss: 0.1012 - val_acc: 0.6871\n",
            "Epoch 8/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.6183Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0996 - acc: 0.7046\n",
            "Epoch 00008: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 139s 705ms/step - loss: 0.1084 - acc: 0.6183 - val_loss: 0.0996 - val_acc: 0.7046\n",
            "Epoch 9/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.6285Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0986 - acc: 0.7080\n",
            "Epoch 00009: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 137s 695ms/step - loss: 0.1067 - acc: 0.6286 - val_loss: 0.0986 - val_acc: 0.7080\n",
            "Epoch 10/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.6394Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0971 - acc: 0.7225\n",
            "Epoch 00010: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 688ms/step - loss: 0.1051 - acc: 0.6395 - val_loss: 0.0971 - val_acc: 0.7225\n",
            "Epoch 11/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.6557Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0965 - acc: 0.7217\n",
            "Epoch 00011: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 135s 685ms/step - loss: 0.1028 - acc: 0.6556 - val_loss: 0.0965 - val_acc: 0.7217\n",
            "Epoch 12/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.6660Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0961 - acc: 0.7240\n",
            "Epoch 00012: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 692ms/step - loss: 0.1011 - acc: 0.6662 - val_loss: 0.0961 - val_acc: 0.7240\n",
            "Epoch 13/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.6759Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0951 - acc: 0.7106\n",
            "Epoch 00013: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 137s 697ms/step - loss: 0.0994 - acc: 0.6758 - val_loss: 0.0951 - val_acc: 0.7106\n",
            "Epoch 14/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.6833Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0911 - acc: 0.7418\n",
            "Epoch 00014: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 138s 700ms/step - loss: 0.0980 - acc: 0.6836 - val_loss: 0.0911 - val_acc: 0.7418\n",
            "Epoch 15/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.6926Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0899 - acc: 0.7411\n",
            "Epoch 00015: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 137s 698ms/step - loss: 0.0962 - acc: 0.6927 - val_loss: 0.0899 - val_acc: 0.7411\n",
            "Epoch 16/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.7004Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0885 - acc: 0.7459\n",
            "Epoch 00016: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 137s 695ms/step - loss: 0.0947 - acc: 0.7005 - val_loss: 0.0885 - val_acc: 0.7459\n",
            "Epoch 17/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.6974Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0878 - acc: 0.7522\n",
            "Epoch 00017: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 690ms/step - loss: 0.0936 - acc: 0.6971 - val_loss: 0.0878 - val_acc: 0.7522\n",
            "Epoch 18/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.7023Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0870 - acc: 0.7541\n",
            "Epoch 00018: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 135s 686ms/step - loss: 0.0923 - acc: 0.7027 - val_loss: 0.0870 - val_acc: 0.7541\n",
            "Epoch 19/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.7120Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0876 - acc: 0.7474\n",
            "Epoch 00019: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 135s 684ms/step - loss: 0.0898 - acc: 0.7117 - val_loss: 0.0876 - val_acc: 0.7474\n",
            "Epoch 20/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.7198Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0838 - acc: 0.7556\n",
            "Epoch 00020: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 691ms/step - loss: 0.0888 - acc: 0.7193 - val_loss: 0.0838 - val_acc: 0.7556\n",
            "Epoch 21/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.7226Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0841 - acc: 0.7600\n",
            "Epoch 00021: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 691ms/step - loss: 0.0874 - acc: 0.7230 - val_loss: 0.0841 - val_acc: 0.7600\n",
            "Epoch 22/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.7260Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0835 - acc: 0.7582\n",
            "Epoch 00022: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 688ms/step - loss: 0.0863 - acc: 0.7259 - val_loss: 0.0835 - val_acc: 0.7582\n",
            "Epoch 23/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.7356Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 34s - loss: 0.0827 - acc: 0.7578\n",
            "Epoch 00023: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 688ms/step - loss: 0.0848 - acc: 0.7360 - val_loss: 0.0827 - val_acc: 0.7578\n",
            "Epoch 24/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.7416Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0801 - acc: 0.7656\n",
            "Epoch 00024: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 136s 689ms/step - loss: 0.0829 - acc: 0.7414 - val_loss: 0.0801 - val_acc: 0.7656\n",
            "Epoch 25/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.7443Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0790 - acc: 0.7634\n",
            "Epoch 00025: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.0818 - acc: 0.7441 - val_loss: 0.0790 - val_acc: 0.7634\n",
            "Epoch 26/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.7430Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0800 - acc: 0.7600\n",
            "Epoch 00026: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 661ms/step - loss: 0.0809 - acc: 0.7429 - val_loss: 0.0800 - val_acc: 0.7600\n",
            "Epoch 27/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.7432Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0772 - acc: 0.7634\n",
            "Epoch 00027: val_loss did not improve from 0.37473\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
            "197/197 [==============================] - 130s 659ms/step - loss: 0.0800 - acc: 0.7437 - val_loss: 0.0772 - val_acc: 0.7634\n",
            "Epoch 28/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.7513Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0772 - acc: 0.7604\n",
            "Epoch 00028: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 660ms/step - loss: 0.0785 - acc: 0.7508 - val_loss: 0.0772 - val_acc: 0.7604\n",
            "Epoch 29/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.7481Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0764 - acc: 0.7649\n",
            "Epoch 00029: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 131s 665ms/step - loss: 0.0781 - acc: 0.7483 - val_loss: 0.0764 - val_acc: 0.7649\n",
            "Epoch 30/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.7521Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0753 - acc: 0.7686\n",
            "Epoch 00030: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 657ms/step - loss: 0.0774 - acc: 0.7524 - val_loss: 0.0753 - val_acc: 0.7686\n",
            "Epoch 31/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.7556Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0750 - acc: 0.7645\n",
            "Epoch 00031: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.0771 - acc: 0.7555 - val_loss: 0.0750 - val_acc: 0.7645\n",
            "Epoch 32/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.7560Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0745 - acc: 0.7701\n",
            "Epoch 00032: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 659ms/step - loss: 0.0763 - acc: 0.7558 - val_loss: 0.0745 - val_acc: 0.7701\n",
            "Epoch 33/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.7664Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 33s - loss: 0.0740 - acc: 0.7638\n",
            "Epoch 00033: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.0752 - acc: 0.7660 - val_loss: 0.0740 - val_acc: 0.7638\n",
            "Epoch 34/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.7592Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0732 - acc: 0.7749\n",
            "Epoch 00034: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 129s 652ms/step - loss: 0.0748 - acc: 0.7596 - val_loss: 0.0732 - val_acc: 0.7749\n",
            "Epoch 35/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.7669Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0723 - acc: 0.7775\n",
            "Epoch 00035: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 658ms/step - loss: 0.0744 - acc: 0.7664 - val_loss: 0.0723 - val_acc: 0.7775\n",
            "Epoch 36/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.7649Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0723 - acc: 0.7760\n",
            "Epoch 00036: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 652ms/step - loss: 0.0737 - acc: 0.7653 - val_loss: 0.0723 - val_acc: 0.7760\n",
            "Epoch 37/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.7681Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0714 - acc: 0.7742\n",
            "Epoch 00037: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.0728 - acc: 0.7684 - val_loss: 0.0714 - val_acc: 0.7742\n",
            "Epoch 38/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.7755Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0712 - acc: 0.7790\n",
            "Epoch 00038: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 650ms/step - loss: 0.0723 - acc: 0.7752 - val_loss: 0.0712 - val_acc: 0.7790\n",
            "Epoch 39/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.7640Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0717 - acc: 0.7656\n",
            "Epoch 00039: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.0721 - acc: 0.7641 - val_loss: 0.0717 - val_acc: 0.7656\n",
            "Epoch 40/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.7645Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0708 - acc: 0.7701\n",
            "Epoch 00040: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 130s 659ms/step - loss: 0.0716 - acc: 0.7649 - val_loss: 0.0708 - val_acc: 0.7701\n",
            "Epoch 41/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.7666Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0690 - acc: 0.7839\n",
            "Epoch 00041: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.0712 - acc: 0.7674 - val_loss: 0.0690 - val_acc: 0.7839\n",
            "Epoch 42/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.7653Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0706 - acc: 0.7708\n",
            "Epoch 00042: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 651ms/step - loss: 0.0710 - acc: 0.7652 - val_loss: 0.0706 - val_acc: 0.7708\n",
            "Epoch 43/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.7845Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0696 - acc: 0.7786\n",
            "Epoch 00043: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.0696 - acc: 0.7849 - val_loss: 0.0696 - val_acc: 0.7786\n",
            "Epoch 44/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.7828Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0685 - acc: 0.7772\n",
            "Epoch 00044: val_loss did not improve from 0.37473\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.0688 - acc: 0.7832 - val_loss: 0.0685 - val_acc: 0.7772\n",
            "Epoch 45/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.7810Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0683 - acc: 0.7775\n",
            "Epoch 00045: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.0686 - acc: 0.7814 - val_loss: 0.0683 - val_acc: 0.7775\n",
            "Epoch 46/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.7745Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0677 - acc: 0.7783\n",
            "Epoch 00046: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 646ms/step - loss: 0.0686 - acc: 0.7752 - val_loss: 0.0677 - val_acc: 0.7783\n",
            "Epoch 47/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.7859Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0673 - acc: 0.7794\n",
            "Epoch 00047: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 638ms/step - loss: 0.0674 - acc: 0.7862 - val_loss: 0.0673 - val_acc: 0.7794\n",
            "Epoch 48/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.7766Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0670 - acc: 0.7816\n",
            "Epoch 00048: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 637ms/step - loss: 0.0679 - acc: 0.7763 - val_loss: 0.0670 - val_acc: 0.7816\n",
            "Epoch 49/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.7795Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0673 - acc: 0.7768\n",
            "Epoch 00049: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0673 - acc: 0.7803 - val_loss: 0.0673 - val_acc: 0.7768\n",
            "Epoch 50/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.7827Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0664 - acc: 0.7865\n",
            "Epoch 00050: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 639ms/step - loss: 0.0671 - acc: 0.7824 - val_loss: 0.0664 - val_acc: 0.7865\n",
            "Epoch 51/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.7760Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0662 - acc: 0.7812\n",
            "Epoch 00051: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0674 - acc: 0.7763 - val_loss: 0.0662 - val_acc: 0.7812\n",
            "Epoch 52/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.7829Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0667 - acc: 0.7749\n",
            "Epoch 00052: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 634ms/step - loss: 0.0666 - acc: 0.7830 - val_loss: 0.0667 - val_acc: 0.7749\n",
            "Epoch 53/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.7914Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0658 - acc: 0.7805\n",
            "Epoch 00053: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 647ms/step - loss: 0.0656 - acc: 0.7916 - val_loss: 0.0658 - val_acc: 0.7805\n",
            "Epoch 54/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.7808Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0657 - acc: 0.7786\n",
            "Epoch 00054: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 637ms/step - loss: 0.0661 - acc: 0.7813 - val_loss: 0.0657 - val_acc: 0.7786\n",
            "Epoch 55/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.7887Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0651 - acc: 0.7872\n",
            "Epoch 00055: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 634ms/step - loss: 0.0655 - acc: 0.7890 - val_loss: 0.0651 - val_acc: 0.7872\n",
            "Epoch 56/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.7904Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0655 - acc: 0.7846\n",
            "Epoch 00056: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0647 - acc: 0.7907 - val_loss: 0.0655 - val_acc: 0.7846\n",
            "Epoch 57/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.7841Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0646 - acc: 0.7883\n",
            "Epoch 00057: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0651 - acc: 0.7841 - val_loss: 0.0646 - val_acc: 0.7883\n",
            "Epoch 58/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.7870Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0648 - acc: 0.7812\n",
            "Epoch 00058: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0646 - acc: 0.7873 - val_loss: 0.0648 - val_acc: 0.7812\n",
            "Epoch 59/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.7955Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0643 - acc: 0.7839\n",
            "Epoch 00059: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 633ms/step - loss: 0.0640 - acc: 0.7958 - val_loss: 0.0643 - val_acc: 0.7839\n",
            "Epoch 60/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.7910Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0647 - acc: 0.7805\n",
            "Epoch 00060: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0640 - acc: 0.7910 - val_loss: 0.0647 - val_acc: 0.7805\n",
            "Epoch 61/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.7938Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0642 - acc: 0.7842\n",
            "Epoch 00061: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 632ms/step - loss: 0.0637 - acc: 0.7939 - val_loss: 0.0642 - val_acc: 0.7842\n",
            "Epoch 62/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.7951Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0632 - acc: 0.7879\n",
            "Epoch 00062: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 123s 626ms/step - loss: 0.0634 - acc: 0.7952 - val_loss: 0.0632 - val_acc: 0.7879\n",
            "Epoch 63/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.7908Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0643 - acc: 0.7846\n",
            "Epoch 00063: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0633 - acc: 0.7909 - val_loss: 0.0643 - val_acc: 0.7846\n",
            "Epoch 64/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.7991Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0633 - acc: 0.7846\n",
            "Epoch 00064: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 123s 624ms/step - loss: 0.0627 - acc: 0.7987 - val_loss: 0.0633 - val_acc: 0.7846\n",
            "Epoch 65/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.7961Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0630 - acc: 0.7842\n",
            "Epoch 00065: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 632ms/step - loss: 0.0625 - acc: 0.7962 - val_loss: 0.0630 - val_acc: 0.7842\n",
            "Epoch 66/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.7946Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0630 - acc: 0.7883\n",
            "Epoch 00066: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 628ms/step - loss: 0.0625 - acc: 0.7937 - val_loss: 0.0630 - val_acc: 0.7883\n",
            "Epoch 67/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.7991Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0625 - acc: 0.7879\n",
            "Epoch 00067: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 628ms/step - loss: 0.0621 - acc: 0.7986 - val_loss: 0.0625 - val_acc: 0.7879\n",
            "Epoch 68/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.7941Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0622 - acc: 0.7872\n",
            "Epoch 00068: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 636ms/step - loss: 0.0619 - acc: 0.7939 - val_loss: 0.0622 - val_acc: 0.7872\n",
            "Epoch 69/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.7914Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0629 - acc: 0.7809\n",
            "Epoch 00069: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 634ms/step - loss: 0.0620 - acc: 0.7916 - val_loss: 0.0629 - val_acc: 0.7809\n",
            "Epoch 70/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.8043Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0623 - acc: 0.7842\n",
            "Epoch 00070: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0611 - acc: 0.8037 - val_loss: 0.0623 - val_acc: 0.7842\n",
            "Epoch 71/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.7899Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0618 - acc: 0.7868\n",
            "Epoch 00071: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0613 - acc: 0.7903 - val_loss: 0.0618 - val_acc: 0.7868\n",
            "Epoch 72/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.8013Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0625 - acc: 0.7812\n",
            "Epoch 00072: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0609 - acc: 0.8010 - val_loss: 0.0625 - val_acc: 0.7812\n",
            "Epoch 73/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.8003Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0616 - acc: 0.7853\n",
            "Epoch 00073: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0607 - acc: 0.8007 - val_loss: 0.0616 - val_acc: 0.7853\n",
            "Epoch 74/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.7936Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0609 - acc: 0.7868\n",
            "Epoch 00074: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0608 - acc: 0.7935 - val_loss: 0.0609 - val_acc: 0.7868\n",
            "Epoch 75/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.8028Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0615 - acc: 0.7868\n",
            "Epoch 00075: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 637ms/step - loss: 0.0602 - acc: 0.8028 - val_loss: 0.0615 - val_acc: 0.7868\n",
            "Epoch 76/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.7995Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0609 - acc: 0.7876\n",
            "Epoch 00076: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 123s 626ms/step - loss: 0.0602 - acc: 0.8000 - val_loss: 0.0609 - val_acc: 0.7876\n",
            "Epoch 77/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.7991Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0607 - acc: 0.7883\n",
            "Epoch 00077: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 631ms/step - loss: 0.0600 - acc: 0.7992 - val_loss: 0.0607 - val_acc: 0.7883\n",
            "Epoch 78/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.8017Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0602 - acc: 0.7876\n",
            "Epoch 00078: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0596 - acc: 0.8024 - val_loss: 0.0602 - val_acc: 0.7876\n",
            "Epoch 79/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.8019Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0598 - acc: 0.7853\n",
            "Epoch 00079: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 629ms/step - loss: 0.0598 - acc: 0.8018 - val_loss: 0.0598 - val_acc: 0.7853\n",
            "Epoch 80/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.8005Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0596 - acc: 0.7883\n",
            "Epoch 00080: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 633ms/step - loss: 0.0595 - acc: 0.8004 - val_loss: 0.0596 - val_acc: 0.7883\n",
            "Epoch 81/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.8032Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0599 - acc: 0.7846\n",
            "Epoch 00081: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 627ms/step - loss: 0.0592 - acc: 0.8032 - val_loss: 0.0599 - val_acc: 0.7846\n",
            "Epoch 82/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.8029Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0594 - acc: 0.7876\n",
            "Epoch 00082: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0586 - acc: 0.8028 - val_loss: 0.0594 - val_acc: 0.7876\n",
            "Epoch 83/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.8077Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0596 - acc: 0.7853\n",
            "Epoch 00083: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0588 - acc: 0.8083 - val_loss: 0.0596 - val_acc: 0.7853\n",
            "Epoch 84/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.8051Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0587 - acc: 0.7961\n",
            "Epoch 00084: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 123s 626ms/step - loss: 0.0585 - acc: 0.8056 - val_loss: 0.0587 - val_acc: 0.7961\n",
            "Epoch 85/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.8075Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0589 - acc: 0.7913\n",
            "Epoch 00085: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0581 - acc: 0.8077 - val_loss: 0.0589 - val_acc: 0.7913\n",
            "Epoch 86/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.8078Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0595 - acc: 0.7842\n",
            "Epoch 00086: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 123s 627ms/step - loss: 0.0580 - acc: 0.8077 - val_loss: 0.0595 - val_acc: 0.7842\n",
            "Epoch 87/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.8059Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0587 - acc: 0.7891\n",
            "Epoch 00087: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 627ms/step - loss: 0.0576 - acc: 0.8058 - val_loss: 0.0587 - val_acc: 0.7891\n",
            "Epoch 88/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.8014Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0587 - acc: 0.7906\n",
            "Epoch 00088: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 630ms/step - loss: 0.0578 - acc: 0.8009 - val_loss: 0.0587 - val_acc: 0.7906\n",
            "Epoch 89/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.8104Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0586 - acc: 0.7913\n",
            "Epoch 00089: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 628ms/step - loss: 0.0573 - acc: 0.8106 - val_loss: 0.0586 - val_acc: 0.7913\n",
            "Epoch 90/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.8138Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0583 - acc: 0.7932\n",
            "Epoch 00090: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 633ms/step - loss: 0.0572 - acc: 0.8142 - val_loss: 0.0583 - val_acc: 0.7932\n",
            "Epoch 91/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.8151Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0591 - acc: 0.7857\n",
            "Epoch 00091: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 124s 628ms/step - loss: 0.0563 - acc: 0.8152 - val_loss: 0.0591 - val_acc: 0.7857\n",
            "Epoch 92/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.8064Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0581 - acc: 0.7902\n",
            "Epoch 00092: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 646ms/step - loss: 0.0569 - acc: 0.8069 - val_loss: 0.0581 - val_acc: 0.7902\n",
            "Epoch 93/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.8197Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0581 - acc: 0.7913\n",
            "Epoch 00093: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.0565 - acc: 0.8200 - val_loss: 0.0581 - val_acc: 0.7913\n",
            "Epoch 94/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.8170Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0578 - acc: 0.7894\n",
            "Epoch 00094: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 642ms/step - loss: 0.0560 - acc: 0.8168 - val_loss: 0.0578 - val_acc: 0.7894\n",
            "Epoch 95/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.8152Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0578 - acc: 0.7928\n",
            "Epoch 00095: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 637ms/step - loss: 0.0560 - acc: 0.8155 - val_loss: 0.0578 - val_acc: 0.7928\n",
            "Epoch 96/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.8199Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0577 - acc: 0.7924\n",
            "Epoch 00096: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 125s 635ms/step - loss: 0.0557 - acc: 0.8200 - val_loss: 0.0577 - val_acc: 0.7924\n",
            "Epoch 97/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.8109Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0572 - acc: 0.7980\n",
            "Epoch 00097: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 643ms/step - loss: 0.0558 - acc: 0.8106 - val_loss: 0.0572 - val_acc: 0.7980\n",
            "Epoch 98/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.8163Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0572 - acc: 0.7954\n",
            "Epoch 00098: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 126s 641ms/step - loss: 0.0556 - acc: 0.8163 - val_loss: 0.0572 - val_acc: 0.7954\n",
            "Epoch 99/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.8087Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 32s - loss: 0.0571 - acc: 0.7924\n",
            "Epoch 00099: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 645ms/step - loss: 0.0561 - acc: 0.8085 - val_loss: 0.0571 - val_acc: 0.7924\n",
            "Epoch 100/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.8155Epoch 1/100\n",
            " 84/197 [===========>..................] - ETA: 31s - loss: 0.0571 - acc: 0.7928\n",
            "Epoch 00100: val_loss did not improve from 0.37473\n",
            "197/197 [==============================] - 127s 642ms/step - loss: 0.0555 - acc: 0.8152 - val_loss: 0.0571 - val_acc: 0.7928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383eb86x9FvQ",
        "colab_type": "text"
      },
      "source": [
        "# **Accuracy and Loss Graph**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKv_2jc7O4Se",
        "colab_type": "text"
      },
      "source": [
        "### **Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtDoga8c10Ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "06fda853-ae93-47a0-ba47-578e317843ee"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6pg2k6xO5p8",
        "colab_type": "text"
      },
      "source": [
        "### **Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1Pr5RmWJkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "dae9d253-8076-486c-c8a9-ae873e40663a"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1cWi7orKdUr",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ea-ST316j3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "067da2b5-9f20-4fc8-8056-9585dd25c5b0"
      },
      "source": [
        "test_data.reset()\n",
        "predictions = model.predict_generator(test_data, steps=test_data.samples/test_data.batch_size,verbose=1)\n",
        "y_pred= np.argmax(predictions, axis=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1002/1002 [==============================] - 15s 15ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJfzBH2gcQ5M",
        "colab_type": "text"
      },
      "source": [
        "### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDJVF1dbWQlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d1a3dfb5-e700-4cc0-bdc3-6634008c51c8"
      },
      "source": [
        "Y_pred = predictions\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(name_as_indexes_test, y_pred))\n",
        "print('Classification Report')\n",
        "classes_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "print(classification_report(name_as_indexes_test, y_pred, target_names=classes_names))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[  9   4  10   0   8   9   1]\n",
            " [  0  25   7   1   5   9   0]\n",
            " [  1   3  80   0  20  21   0]\n",
            " [  0   0   5   4   2   4   0]\n",
            " [  0   0  14   0  64  31   1]\n",
            " [  0   1  24   1  22 600   1]\n",
            " [  0   0   1   0   0   1  13]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec       0.90      0.22      0.35        41\n",
            "         bcc       0.76      0.53      0.62        47\n",
            "         bkl       0.57      0.64      0.60       125\n",
            "          df       0.67      0.27      0.38        15\n",
            "         mel       0.53      0.58      0.55       110\n",
            "          nv       0.89      0.92      0.91       649\n",
            "        vasc       0.81      0.87      0.84        15\n",
            "\n",
            "    accuracy                           0.79      1002\n",
            "   macro avg       0.73      0.58      0.61      1002\n",
            "weighted avg       0.80      0.79      0.78      1002\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMxOe1qPcUSr",
        "colab_type": "text"
      },
      "source": [
        "### **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWSBDzE5WXgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "78bdaeb2-0af3-444e-adab-10a73abeec5e"
      },
      "source": [
        "cm = (confusion_matrix(name_as_indexes_test, y_pred))\n",
        "\n",
        "plot_confusion_matrix(cm, classes_names)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[  9   4  10   0   8   9   1]\n",
            " [  0  25   7   1   5   9   0]\n",
            " [  1   3  80   0  20  21   0]\n",
            " [  0   0   5   4   2   4   0]\n",
            " [  0   0  14   0  64  31   1]\n",
            " [  0   1  24   1  22 600   1]\n",
            " [  0   0   1   0   0   1  13]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXcAwdJ-1tX",
        "colab_type": "text"
      },
      "source": [
        "### **Sensitivity & Specificity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH-wStPm6T7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "total = sum(sum(cm))\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc7FeOe56a9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e926f5e-c4d5-487f-b4d3-0064bd008553"
      },
      "source": [
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sensitivity: 0.6923\n",
            "specificity: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrPLbkZdva9",
        "colab_type": "text"
      },
      "source": [
        "# **Grad-CAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_OVC2F8xfR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_conv2d = 'conv5_block3_3_conv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyWQAabav_ST",
        "colab_type": "text"
      },
      "source": [
        "### **AKIEC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5JjxhIrqN5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "akiec_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0026492.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGMmNOxxiHxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, akiec_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53Hg3-mwNj-",
        "colab_type": "text"
      },
      "source": [
        "### **BCC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-APRYghPa4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bcc_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024332.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IavEoR0qwWO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, bcc_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnzfOod3xC7p",
        "colab_type": "text"
      },
      "source": [
        "### **BKL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Za1Ztu0kxVpc",
        "colab": {}
      },
      "source": [
        "bkl_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0025548.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xc_tDIcsxVpg",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, bkl_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2qr1BF9xEnD",
        "colab_type": "text"
      },
      "source": [
        "### **DF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w3400LGvxWEF",
        "colab": {}
      },
      "source": [
        "df_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0033626.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S92MquPFxWEI",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, df_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y6JOyRWxJy7",
        "colab_type": "text"
      },
      "source": [
        "### **MEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mRW-epcxWdq",
        "colab": {}
      },
      "source": [
        "mel_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024516.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g4j2KO_pxWdv",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, mel_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKEm-_WKxHRq",
        "colab_type": "text"
      },
      "source": [
        "### **NV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opRTYW5SxXgR",
        "colab": {}
      },
      "source": [
        "nv_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024349.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iVbqIHvGxXgV",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, nv_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVFPRGL0xHtK",
        "colab_type": "text"
      },
      "source": [
        "### **VASC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YuzSPvQ9xXJg",
        "colab": {}
      },
      "source": [
        "vasc_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0025452.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ol1Ik8qDxXJj",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, vasc_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYJYt3yOB47l",
        "colab_type": "text"
      },
      "source": [
        "# **Download Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFv5jXItB-fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuVkAwOn3A9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/Focal-Loss_ResNet50_model.h5')\n",
        "files.download('/content/Focal-Loss_ResNet50_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}