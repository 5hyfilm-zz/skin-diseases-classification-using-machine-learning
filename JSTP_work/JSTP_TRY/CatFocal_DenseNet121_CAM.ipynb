{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatFocal_DenseNet121_CAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPcOufvvzdXEujdQi4iIZx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filmerxyz/JSTP-22_SkinDiseaseClassificationUsingMachineLearning/blob/master/CatFocal_DenseNet121_CAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGAbDjt3rz_5",
        "colab_type": "text"
      },
      "source": [
        "# **Check GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KeAnM3PXlUe",
        "colab_type": "code",
        "outputId": "4c48cd9d-7093-4817-b0ae-23b63be927f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 31 18:10:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbD3Qq816yZZ",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tch5If72HQeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "from ham10000_utils_functions import plot_confusion_matrix, normalize, deprocess_image, my_decode_predictions, guided_backprop, grad_cam, compute_saliency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPwDTos761Oq",
        "colab_type": "text"
      },
      "source": [
        "# **Clone Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjfNRvqza-U",
        "colab_type": "code",
        "outputId": "e780ccd8-1084-4580-9998-6b0a6e13e6e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/EvilPickle-PCSHSPT/ham10000-with-one-image-folder"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ham10000-with-one-image-folder' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__IXKxBZVymL",
        "colab_type": "text"
      },
      "source": [
        "# **Constant Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ2LKSeHQRnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 7\n",
        "STEPS = 16\n",
        "\n",
        "LR = 3e-5 # Learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrpBmL7mVlJC",
        "colab_type": "text"
      },
      "source": [
        "# **Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnsBJBcPL3SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('/content/ham10000-with-one-image-folder/HAM10000_metadata.csv')\n",
        "data['image_full_name']=data['image_id']+'.jpg'\n",
        "X=data[['image_full_name','dx','lesion_id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rt4v6YTM0Fr",
        "colab_type": "code",
        "outputId": "d490dbe3-b69e-4761-c1b7-4c9d708b674c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "      <th>image_full_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0027419</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0027419.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAM_0000118</td>\n",
              "      <td>ISIC_0025030</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0025030.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0026769</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0026769.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HAM_0002730</td>\n",
              "      <td>ISIC_0025661</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>80.0</td>\n",
              "      <td>male</td>\n",
              "      <td>scalp</td>\n",
              "      <td>ISIC_0025661.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HAM_0001466</td>\n",
              "      <td>ISIC_0031633</td>\n",
              "      <td>bkl</td>\n",
              "      <td>histo</td>\n",
              "      <td>75.0</td>\n",
              "      <td>male</td>\n",
              "      <td>ear</td>\n",
              "      <td>ISIC_0031633.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lesion_id      image_id   dx  ...   sex  localization   image_full_name\n",
              "0  HAM_0000118  ISIC_0027419  bkl  ...  male         scalp  ISIC_0027419.jpg\n",
              "1  HAM_0000118  ISIC_0025030  bkl  ...  male         scalp  ISIC_0025030.jpg\n",
              "2  HAM_0002730  ISIC_0026769  bkl  ...  male         scalp  ISIC_0026769.jpg\n",
              "3  HAM_0002730  ISIC_0025661  bkl  ...  male         scalp  ISIC_0025661.jpg\n",
              "4  HAM_0001466  ISIC_0031633  bkl  ...  male           ear  ISIC_0031633.jpg\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_1JsUvHGMGi",
        "colab_type": "text"
      },
      "source": [
        "### **Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1q1WsAhM-HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y=X.pop('dx').to_frame()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ghodz0zOJ0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.concat([X_train,y_train],axis=1)\n",
        "val = pd.concat([X_val,y_val],axis=1)\n",
        "test = pd.concat([X_test,y_test],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG6iaiiyMHmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train['dx'])\n",
        "name_as_indexes_train = encoder.transform(train['dx']) \n",
        "train['label'] = name_as_indexes_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLryd9huOStO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(val['dx'])\n",
        "name_as_indexes_val = encoder.transform(val['dx']) \n",
        "val['label'] = name_as_indexes_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VYdnvBOOUek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder=LabelEncoder()\n",
        "encoder.fit(test['dx'])\n",
        "name_as_indexes_test = encoder.transform(test['dx']) \n",
        "test['label'] = name_as_indexes_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDOtGZx7YiJa",
        "colab_type": "text"
      },
      "source": [
        "### **Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6snNZRZOWaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = ImageDataGenerator(rescale = 1./255,\n",
        "                                     rotation_range=360,  \n",
        "                                     zoom_range = 0.3,\n",
        "                                     horizontal_flip=True,\n",
        "                                     vertical_flip=True,\n",
        "                                     fill_mode='reflect')\n",
        "                                    \n",
        "val_generator=ImageDataGenerator(rescale = 1./255)\n",
        "test_generator=ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdF3KvYwOfC5",
        "colab_type": "code",
        "outputId": "826d36e7-82fa-4cd2-9cca-9cb7a4f99eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data= train_generator.flow_from_dataframe(dataframe=train, x_col=\"image_full_name\", y_col=\"dx\",\n",
        "                                                directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                                shuffle=True,batch_size=32,class_mode=\"categorical\",target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "val_data= val_generator.flow_from_dataframe(dataframe=val, x_col=\"image_full_name\", y_col=\"dx\",\n",
        "                                              directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                              shuffle=True,batch_size=32,class_mode='categorical',target_size=(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "test_data= test_generator.flow_from_dataframe(dataframe=test, x_col=\"image_full_name\", y_col=\"dx\",\n",
        "                                              directory='/content/ham10000-with-one-image-folder/HAM1000_images',\n",
        "                                              shuffle=False,batch_size=1,class_mode=None,target_size=(IMG_WIDTH,IMG_HEIGHT))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6309 validated image filenames belonging to 7 classes.\n",
            "Found 2704 validated image filenames belonging to 7 classes.\n",
            "Found 1002 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WOs0EjX8Ot2",
        "colab_type": "text"
      },
      "source": [
        "# **Focal Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9H8QLiwJ_rc",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/umbertogriffo/focal-loss-keras\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he4CnLjdH8c5",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvORsbwy69hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss.\n",
        "           m\n",
        "      FL = âˆ‘  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
        "          c=1\n",
        "      where m = number of classes, c = class and o = observation\n",
        "    Parameters:\n",
        "      alpha -- the same as weighing factor in balanced cross entropy\n",
        "      gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "      gamma -- 2.0 as mentioned in the paper\n",
        "      alpha -- 0.25 as mentioned in the paper\n",
        "    References:\n",
        "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
        "    Usage:\n",
        "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred: A tensor resulting from a softmax\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        # Scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "        # Clip the prediction value to prevent NaN's and Inf's\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calculate Cross Entropy\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "        # Calculate Focal Loss\n",
        "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Compute mean loss in mini_batch\n",
        "        return K.mean(loss, axis=1)\n",
        "\n",
        "    return categorical_focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM_7koKYSwRX",
        "colab_type": "text"
      },
      "source": [
        "# Class Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqppou6MSvrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(name_as_indexes_train), name_as_indexes_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhjJOtHPTOqN",
        "colab_type": "code",
        "outputId": "48962c43-fc58-457f-eae8-99c5bb1e7bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "class_weights"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.76870748,  2.82534707,  1.32542017, 11.40867993,  1.28939301,\n",
              "        0.21181803, 10.24188312])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoJAN5opTvV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_class_weight_dict = { i : class_weights[i] for i in range(0, len(class_weights) ) }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nM6ctD3Ty8H",
        "colab_type": "code",
        "outputId": "1e9c1019-6c47-4da0-98c7-35d6f734fb7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_class_weight_dict"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 4.7687074829931975,\n",
              " 1: 2.825347066726377,\n",
              " 2: 1.3254201680672268,\n",
              " 3: 11.408679927667269,\n",
              " 4: 1.2893930104230533,\n",
              " 5: 0.21181802920933357,\n",
              " 6: 10.241883116883116}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivQqzr9X8T8Z",
        "colab_type": "text"
      },
      "source": [
        "# **Build Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6NKOM5-VHcm",
        "colab_type": "text"
      },
      "source": [
        "### **Use DenseNet121 + fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982Ib3LtLZ19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  base_model = tf.keras.applications.DenseNet121(include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), weights='imagenet')\n",
        "  \n",
        "  for layer in base_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "  average_pooling_layer = GlobalAveragePooling2D()(base_model.output)\n",
        "  \n",
        "  fc_layer = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(average_pooling_layer)\n",
        "  fc_layer = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(fc_layer)\n",
        "  fc_layer = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(fc_layer)\n",
        "  fc_layer = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(fc_layer)\n",
        "  fc_layer = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(fc_layer)\n",
        "  bn_layer = BatchNormalization()(fc_layer)\n",
        "  dropout_layer = Dropout(0.25)(bn_layer)\n",
        "  prediction_layer = Dense(units=7, activation='softmax', name='prediction')(dropout_layer)\n",
        "  model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
        "  \n",
        "  model.compile(optimizer=Adam(LR), loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quSBa9F3Qdqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TFNv-FGLdgd",
        "colab_type": "text"
      },
      "source": [
        "### **Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1bAKncT_-0g",
        "colab_type": "code",
        "outputId": "d7c06d62-4b66-47a1-b86e-1298865d9f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          524800      global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 32)           2080        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32)           128         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Dense)              (None, 7)            231         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,737,223\n",
            "Trainable params: 783,303\n",
            "Non-trainable params: 6,953,920\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUkpZH2iO2HT",
        "colab_type": "text"
      },
      "source": [
        "### **Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W01_QL-DQujb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/SigFL_DenseNet121_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-TRuxDhSBVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1,\n",
        "                             save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPtUnf9tSCYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnTxHeV9ym7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_plateau = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=.5, min_lr=1-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGLVL4QTSDOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cb_list = [checkpoint, early_stop, reduce_plateau]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tWxLCM5VFYz",
        "colab_type": "text"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-tzC-r3SEBp",
        "colab_type": "code",
        "outputId": "202eabaf-1066-4955-8e4a-144a9c4a224a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train_data,\n",
        "                    steps_per_epoch=train_data.samples//train_data.batch_size,\n",
        "                    validation_data=val_data,\n",
        "                    validation_steps=val_data.samples//val_data.batch_size,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=cb_list,\n",
        "                    verbose=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 1.3133 - accuracy: 0.2007\n",
            "Epoch 00001: val_loss improved from -inf to 1.23424, saving model to /content/SigFL_DenseNet121_model.h5\n",
            "197/197 [==============================] - 136s 691ms/step - loss: 1.3133 - accuracy: 0.2007 - val_loss: 1.2342 - val_accuracy: 0.2433 - lr: 3.0000e-05\n",
            "Epoch 2/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 1.1733 - accuracy: 0.3841\n",
            "Epoch 00002: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 675ms/step - loss: 1.1733 - accuracy: 0.3841 - val_loss: 1.1010 - val_accuracy: 0.5964 - lr: 3.0000e-05\n",
            "Epoch 3/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 1.0562 - accuracy: 0.4857\n",
            "Epoch 00003: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 131s 667ms/step - loss: 1.0562 - accuracy: 0.4857 - val_loss: 0.9917 - val_accuracy: 0.6544 - lr: 3.0000e-05\n",
            "Epoch 4/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.5503\n",
            "Epoch 00004: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 657ms/step - loss: 0.9537 - accuracy: 0.5503 - val_loss: 0.8962 - val_accuracy: 0.6838 - lr: 3.0000e-05\n",
            "Epoch 5/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.8626 - accuracy: 0.6078\n",
            "Epoch 00005: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 658ms/step - loss: 0.8626 - accuracy: 0.6078 - val_loss: 0.8114 - val_accuracy: 0.7173 - lr: 3.0000e-05\n",
            "Epoch 6/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.6385\n",
            "Epoch 00006: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 661ms/step - loss: 0.7818 - accuracy: 0.6385 - val_loss: 0.7374 - val_accuracy: 0.7128 - lr: 3.0000e-05\n",
            "Epoch 7/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.6634\n",
            "Epoch 00007: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.7101 - accuracy: 0.6634 - val_loss: 0.6728 - val_accuracy: 0.7117 - lr: 3.0000e-05\n",
            "Epoch 8/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.6830\n",
            "Epoch 00008: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.6463 - accuracy: 0.6830 - val_loss: 0.6133 - val_accuracy: 0.7158 - lr: 3.0000e-05\n",
            "Epoch 9/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.6897\n",
            "Epoch 00009: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 651ms/step - loss: 0.6028 - accuracy: 0.6897 - val_loss: 0.5838 - val_accuracy: 0.7269 - lr: 1.5000e-05\n",
            "Epoch 10/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.7077\n",
            "Epoch 00010: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 652ms/step - loss: 0.5745 - accuracy: 0.7077 - val_loss: 0.5570 - val_accuracy: 0.7340 - lr: 1.5000e-05\n",
            "Epoch 11/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.7108\n",
            "Epoch 00011: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 650ms/step - loss: 0.5481 - accuracy: 0.7108 - val_loss: 0.5313 - val_accuracy: 0.7336 - lr: 1.5000e-05\n",
            "Epoch 12/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.7145\n",
            "Epoch 00012: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 652ms/step - loss: 0.5230 - accuracy: 0.7145 - val_loss: 0.5068 - val_accuracy: 0.7392 - lr: 1.5000e-05\n",
            "Epoch 13/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.7196\n",
            "Epoch 00013: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 650ms/step - loss: 0.4993 - accuracy: 0.7196 - val_loss: 0.4843 - val_accuracy: 0.7429 - lr: 1.5000e-05\n",
            "Epoch 14/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.7210\n",
            "Epoch 00014: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 650ms/step - loss: 0.4769 - accuracy: 0.7210 - val_loss: 0.4624 - val_accuracy: 0.7433 - lr: 1.5000e-05\n",
            "Epoch 15/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.7303\n",
            "Epoch 00015: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.4550 - accuracy: 0.7303 - val_loss: 0.4412 - val_accuracy: 0.7463 - lr: 1.5000e-05\n",
            "Epoch 16/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.7339\n",
            "Epoch 00016: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 648ms/step - loss: 0.4351 - accuracy: 0.7339 - val_loss: 0.4216 - val_accuracy: 0.7478 - lr: 1.5000e-05\n",
            "Epoch 17/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.7381\n",
            "Epoch 00017: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.4156 - accuracy: 0.7381 - val_loss: 0.4034 - val_accuracy: 0.7526 - lr: 1.5000e-05\n",
            "Epoch 18/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.7349\n",
            "Epoch 00018: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 662ms/step - loss: 0.3974 - accuracy: 0.7349 - val_loss: 0.3858 - val_accuracy: 0.7563 - lr: 1.5000e-05\n",
            "Epoch 19/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.7383\n",
            "Epoch 00019: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 650ms/step - loss: 0.3805 - accuracy: 0.7383 - val_loss: 0.3692 - val_accuracy: 0.7571 - lr: 1.5000e-05\n",
            "Epoch 20/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.7518\n",
            "Epoch 00020: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.3645 - accuracy: 0.7518 - val_loss: 0.3537 - val_accuracy: 0.7604 - lr: 1.5000e-05\n",
            "Epoch 21/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.7510\n",
            "Epoch 00021: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 652ms/step - loss: 0.3488 - accuracy: 0.7510 - val_loss: 0.3395 - val_accuracy: 0.7727 - lr: 1.5000e-05\n",
            "Epoch 22/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.7486\n",
            "Epoch 00022: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.3345 - accuracy: 0.7486 - val_loss: 0.3246 - val_accuracy: 0.7716 - lr: 1.5000e-05\n",
            "Epoch 23/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.7606\n",
            "Epoch 00023: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.3207 - accuracy: 0.7606 - val_loss: 0.3122 - val_accuracy: 0.7578 - lr: 1.5000e-05\n",
            "Epoch 24/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.7569\n",
            "Epoch 00024: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.3078 - accuracy: 0.7569 - val_loss: 0.3001 - val_accuracy: 0.7697 - lr: 1.5000e-05\n",
            "Epoch 25/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.7660\n",
            "Epoch 00025: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2983 - accuracy: 0.7660 - val_loss: 0.2936 - val_accuracy: 0.7667 - lr: 7.5000e-06\n",
            "Epoch 26/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.7666\n",
            "Epoch 00026: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2927 - accuracy: 0.7666 - val_loss: 0.2881 - val_accuracy: 0.7753 - lr: 7.5000e-06\n",
            "Epoch 27/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.7615\n",
            "Epoch 00027: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.2865 - accuracy: 0.7615 - val_loss: 0.2822 - val_accuracy: 0.7690 - lr: 7.5000e-06\n",
            "Epoch 28/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.7771\n",
            "Epoch 00028: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2806 - accuracy: 0.7771 - val_loss: 0.2777 - val_accuracy: 0.7679 - lr: 7.5000e-06\n",
            "Epoch 29/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.7660\n",
            "Epoch 00029: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 652ms/step - loss: 0.2755 - accuracy: 0.7660 - val_loss: 0.2711 - val_accuracy: 0.7798 - lr: 7.5000e-06\n",
            "Epoch 30/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.7696\n",
            "Epoch 00030: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 657ms/step - loss: 0.2698 - accuracy: 0.7696 - val_loss: 0.2655 - val_accuracy: 0.7749 - lr: 7.5000e-06\n",
            "Epoch 31/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.7744\n",
            "Epoch 00031: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 657ms/step - loss: 0.2641 - accuracy: 0.7744 - val_loss: 0.2602 - val_accuracy: 0.7805 - lr: 7.5000e-06\n",
            "Epoch 32/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.7806\n",
            "Epoch 00032: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 657ms/step - loss: 0.2590 - accuracy: 0.7806 - val_loss: 0.2560 - val_accuracy: 0.7783 - lr: 7.5000e-06\n",
            "Epoch 33/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.7728\n",
            "Epoch 00033: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.2545 - accuracy: 0.7728 - val_loss: 0.2510 - val_accuracy: 0.7757 - lr: 7.5000e-06\n",
            "Epoch 34/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.7768\n",
            "Epoch 00034: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.2493 - accuracy: 0.7768 - val_loss: 0.2458 - val_accuracy: 0.7820 - lr: 7.5000e-06\n",
            "Epoch 35/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.7774\n",
            "Epoch 00035: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.2449 - accuracy: 0.7774 - val_loss: 0.2412 - val_accuracy: 0.7850 - lr: 7.5000e-06\n",
            "Epoch 36/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.7760\n",
            "Epoch 00036: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 128s 652ms/step - loss: 0.2398 - accuracy: 0.7760 - val_loss: 0.2369 - val_accuracy: 0.7775 - lr: 7.5000e-06\n",
            "Epoch 37/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.7771\n",
            "Epoch 00037: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.2352 - accuracy: 0.7771 - val_loss: 0.2323 - val_accuracy: 0.7824 - lr: 7.5000e-06\n",
            "Epoch 38/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.7808\n",
            "Epoch 00038: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.2311 - accuracy: 0.7808 - val_loss: 0.2279 - val_accuracy: 0.7865 - lr: 7.5000e-06\n",
            "Epoch 39/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.7829\n",
            "Epoch 00039: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2267 - accuracy: 0.7829 - val_loss: 0.2238 - val_accuracy: 0.7902 - lr: 7.5000e-06\n",
            "Epoch 40/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.7867\n",
            "Epoch 00040: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 659ms/step - loss: 0.2223 - accuracy: 0.7867 - val_loss: 0.2196 - val_accuracy: 0.7879 - lr: 7.5000e-06\n",
            "Epoch 41/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.7760\n",
            "Epoch 00041: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 657ms/step - loss: 0.2185 - accuracy: 0.7760 - val_loss: 0.2159 - val_accuracy: 0.7920 - lr: 7.5000e-06\n",
            "Epoch 42/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.7888\n",
            "Epoch 00042: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.2143 - accuracy: 0.7888 - val_loss: 0.2119 - val_accuracy: 0.7902 - lr: 7.5000e-06\n",
            "Epoch 43/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.7786\n",
            "Epoch 00043: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2109 - accuracy: 0.7786 - val_loss: 0.2082 - val_accuracy: 0.7853 - lr: 7.5000e-06\n",
            "Epoch 44/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.7854\n",
            "Epoch 00044: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2073 - accuracy: 0.7854 - val_loss: 0.2044 - val_accuracy: 0.7883 - lr: 7.5000e-06\n",
            "Epoch 45/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.7888\n",
            "Epoch 00045: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.2043 - accuracy: 0.7888 - val_loss: 0.2028 - val_accuracy: 0.7894 - lr: 3.7500e-06\n",
            "Epoch 46/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.7875\n",
            "Epoch 00046: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.2029 - accuracy: 0.7875 - val_loss: 0.2012 - val_accuracy: 0.7917 - lr: 3.7500e-06\n",
            "Epoch 47/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.7934\n",
            "Epoch 00047: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.2005 - accuracy: 0.7934 - val_loss: 0.1993 - val_accuracy: 0.7958 - lr: 3.7500e-06\n",
            "Epoch 48/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.7956\n",
            "Epoch 00048: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1989 - accuracy: 0.7956 - val_loss: 0.1977 - val_accuracy: 0.7939 - lr: 3.7500e-06\n",
            "Epoch 49/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.7900\n",
            "Epoch 00049: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 130s 658ms/step - loss: 0.1974 - accuracy: 0.7900 - val_loss: 0.1958 - val_accuracy: 0.7972 - lr: 3.7500e-06\n",
            "Epoch 50/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.7895\n",
            "Epoch 00050: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1956 - accuracy: 0.7895 - val_loss: 0.1943 - val_accuracy: 0.7935 - lr: 3.7500e-06\n",
            "Epoch 51/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.7886\n",
            "Epoch 00051: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1939 - accuracy: 0.7886 - val_loss: 0.1933 - val_accuracy: 0.7917 - lr: 3.7500e-06\n",
            "Epoch 52/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.7900\n",
            "Epoch 00052: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.1923 - accuracy: 0.7900 - val_loss: 0.1908 - val_accuracy: 0.7969 - lr: 3.7500e-06\n",
            "Epoch 53/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.7918\n",
            "Epoch 00053: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.1913 - accuracy: 0.7918 - val_loss: 0.1901 - val_accuracy: 0.7972 - lr: 1.8750e-06\n",
            "Epoch 54/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.7899\n",
            "Epoch 00054: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 656ms/step - loss: 0.1902 - accuracy: 0.7899 - val_loss: 0.1894 - val_accuracy: 0.7972 - lr: 1.8750e-06\n",
            "Epoch 55/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.7883\n",
            "Epoch 00055: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.37499976316758e-07.\n",
            "197/197 [==============================] - 128s 652ms/step - loss: 0.1900 - accuracy: 0.7883 - val_loss: 0.1887 - val_accuracy: 0.7939 - lr: 1.8750e-06\n",
            "Epoch 56/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.7956\n",
            "Epoch 00056: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 657ms/step - loss: 0.1887 - accuracy: 0.7956 - val_loss: 0.1881 - val_accuracy: 0.7950 - lr: 9.3750e-07\n",
            "Epoch 57/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.7908\n",
            "Epoch 00057: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.1889 - accuracy: 0.7908 - val_loss: 0.1878 - val_accuracy: 0.7950 - lr: 9.3750e-07\n",
            "Epoch 58/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.7951\n",
            "Epoch 00058: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 4.68749988158379e-07.\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1883 - accuracy: 0.7951 - val_loss: 0.1875 - val_accuracy: 0.7939 - lr: 9.3750e-07\n",
            "Epoch 59/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.7931\n",
            "Epoch 00059: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 654ms/step - loss: 0.1875 - accuracy: 0.7931 - val_loss: 0.1875 - val_accuracy: 0.7958 - lr: 4.6875e-07\n",
            "Epoch 60/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.7878\n",
            "Epoch 00060: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 653ms/step - loss: 0.1879 - accuracy: 0.7878 - val_loss: 0.1872 - val_accuracy: 0.7954 - lr: 4.6875e-07\n",
            "Epoch 61/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.7911\n",
            "Epoch 00061: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.343749940791895e-07.\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1876 - accuracy: 0.7911 - val_loss: 0.1870 - val_accuracy: 0.7954 - lr: 4.6875e-07\n",
            "Epoch 62/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.7956\n",
            "Epoch 00062: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 129s 655ms/step - loss: 0.1872 - accuracy: 0.7956 - val_loss: 0.1867 - val_accuracy: 0.7972 - lr: 2.3437e-07\n",
            "Epoch 63/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.7897\n",
            "Epoch 00063: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 670ms/step - loss: 0.1876 - accuracy: 0.7897 - val_loss: 0.1869 - val_accuracy: 0.7932 - lr: 2.3437e-07\n",
            "Epoch 64/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.8029\n",
            "Epoch 00064: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.1718749703959475e-07.\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1868 - accuracy: 0.8029 - val_loss: 0.1865 - val_accuracy: 0.7946 - lr: 2.3437e-07\n",
            "Epoch 65/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7985\n",
            "Epoch 00065: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1867 - accuracy: 0.7985 - val_loss: 0.1865 - val_accuracy: 0.7954 - lr: 1.1719e-07\n",
            "Epoch 66/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.7888\n",
            "Epoch 00066: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1871 - accuracy: 0.7888 - val_loss: 0.1867 - val_accuracy: 0.7935 - lr: 1.1719e-07\n",
            "Epoch 67/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.7946\n",
            "Epoch 00067: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 5.859374851979737e-08.\n",
            "197/197 [==============================] - 132s 671ms/step - loss: 0.1868 - accuracy: 0.7946 - val_loss: 0.1865 - val_accuracy: 0.7969 - lr: 1.1719e-07\n",
            "Epoch 68/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7942\n",
            "Epoch 00068: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1867 - accuracy: 0.7942 - val_loss: 0.1864 - val_accuracy: 0.7954 - lr: 5.8594e-08\n",
            "Epoch 69/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.7921\n",
            "Epoch 00069: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1870 - accuracy: 0.7921 - val_loss: 0.1864 - val_accuracy: 0.7950 - lr: 5.8594e-08\n",
            "Epoch 70/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.7924\n",
            "Epoch 00070: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 2.9296874259898686e-08.\n",
            "197/197 [==============================] - 132s 671ms/step - loss: 0.1869 - accuracy: 0.7924 - val_loss: 0.1864 - val_accuracy: 0.7954 - lr: 5.8594e-08\n",
            "Epoch 71/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7907\n",
            "Epoch 00071: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 668ms/step - loss: 0.1867 - accuracy: 0.7907 - val_loss: 0.1866 - val_accuracy: 0.7939 - lr: 2.9297e-08\n",
            "Epoch 72/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.7950\n",
            "Epoch 00072: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1868 - accuracy: 0.7950 - val_loss: 0.1863 - val_accuracy: 0.7950 - lr: 2.9297e-08\n",
            "Epoch 73/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.7883\n",
            "Epoch 00073: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.4648437129949343e-08.\n",
            "197/197 [==============================] - 132s 671ms/step - loss: 0.1871 - accuracy: 0.7883 - val_loss: 0.1864 - val_accuracy: 0.7950 - lr: 2.9297e-08\n",
            "Epoch 74/100\n",
            "196/197 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.7956\n",
            "Epoch 00074: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 668ms/step - loss: 0.1866 - accuracy: 0.7956 - val_loss: 0.1864 - val_accuracy: 0.7958 - lr: 1.4648e-08\n",
            "Epoch 75/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.7916\n",
            "Epoch 00075: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 673ms/step - loss: 0.1866 - accuracy: 0.7916 - val_loss: 0.1863 - val_accuracy: 0.7943 - lr: 1.4648e-08\n",
            "Epoch 76/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.7929\n",
            "Epoch 00076: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 7.324218564974672e-09.\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1868 - accuracy: 0.7929 - val_loss: 0.1863 - val_accuracy: 0.7965 - lr: 1.4648e-08\n",
            "Epoch 77/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7931\n",
            "Epoch 00077: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1867 - accuracy: 0.7931 - val_loss: 0.1864 - val_accuracy: 0.7958 - lr: 7.3242e-09\n",
            "Epoch 78/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.7966\n",
            "Epoch 00078: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1868 - accuracy: 0.7966 - val_loss: 0.1864 - val_accuracy: 0.7946 - lr: 7.3242e-09\n",
            "Epoch 79/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.7970\n",
            "Epoch 00079: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.662109282487336e-09.\n",
            "197/197 [==============================] - 131s 667ms/step - loss: 0.1864 - accuracy: 0.7970 - val_loss: 0.1865 - val_accuracy: 0.7932 - lr: 7.3242e-09\n",
            "Epoch 80/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.7994\n",
            "Epoch 00080: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1863 - accuracy: 0.7994 - val_loss: 0.1863 - val_accuracy: 0.7972 - lr: 3.6621e-09\n",
            "Epoch 81/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.7977\n",
            "Epoch 00081: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 673ms/step - loss: 0.1864 - accuracy: 0.7977 - val_loss: 0.1862 - val_accuracy: 0.7958 - lr: 3.6621e-09\n",
            "Epoch 82/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7958\n",
            "Epoch 00082: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.831054641243668e-09.\n",
            "197/197 [==============================] - 133s 675ms/step - loss: 0.1867 - accuracy: 0.7958 - val_loss: 0.1864 - val_accuracy: 0.7950 - lr: 3.6621e-09\n",
            "Epoch 83/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.8005\n",
            "Epoch 00083: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1865 - accuracy: 0.8005 - val_loss: 0.1864 - val_accuracy: 0.7932 - lr: 1.8311e-09\n",
            "Epoch 84/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.7915\n",
            "Epoch 00084: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 673ms/step - loss: 0.1868 - accuracy: 0.7915 - val_loss: 0.1864 - val_accuracy: 0.7954 - lr: 1.8311e-09\n",
            "Epoch 85/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.7975\n",
            "Epoch 00085: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 9.15527320621834e-10.\n",
            "197/197 [==============================] - 132s 670ms/step - loss: 0.1866 - accuracy: 0.7975 - val_loss: 0.1864 - val_accuracy: 0.7946 - lr: 1.8311e-09\n",
            "Epoch 86/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.7918\n",
            "Epoch 00086: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 674ms/step - loss: 0.1871 - accuracy: 0.7918 - val_loss: 0.1863 - val_accuracy: 0.7965 - lr: 9.1553e-10\n",
            "Epoch 87/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.7911\n",
            "Epoch 00087: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 672ms/step - loss: 0.1865 - accuracy: 0.7911 - val_loss: 0.1865 - val_accuracy: 0.7946 - lr: 9.1553e-10\n",
            "Epoch 88/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.7983\n",
            "Epoch 00088: val_loss did not improve from 1.23424\n",
            "\n",
            "Epoch 00088: ReduceLROnPlateau reducing learning rate to 4.57763660310917e-10.\n",
            "197/197 [==============================] - 132s 670ms/step - loss: 0.1867 - accuracy: 0.7983 - val_loss: 0.1863 - val_accuracy: 0.7972 - lr: 9.1553e-10\n",
            "Epoch 89/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.7993\n",
            "Epoch 00089: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 133s 674ms/step - loss: 0.1865 - accuracy: 0.7993 - val_loss: 0.1862 - val_accuracy: 0.7961 - lr: 4.5776e-10\n",
            "Epoch 90/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.7959\n",
            "Epoch 00090: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 131s 665ms/step - loss: 0.1869 - accuracy: 0.7959 - val_loss: 0.1864 - val_accuracy: 0.7980 - lr: 4.5776e-10\n",
            "Epoch 91/100\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.7967\n",
            "Epoch 00091: val_loss did not improve from 1.23424\n",
            "197/197 [==============================] - 132s 669ms/step - loss: 0.1869 - accuracy: 0.7967 - val_loss: 0.1862 - val_accuracy: 0.7950 - lr: 4.5776e-10\n",
            "Epoch 00091: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383eb86x9FvQ",
        "colab_type": "text"
      },
      "source": [
        "# **Accuracy and Loss Graph**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKv_2jc7O4Se",
        "colab_type": "text"
      },
      "source": [
        "### **Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtDoga8c10Ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7ddee7e0-b3f9-49d0-a44b-79e825a91daa"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3hc9ZXw8e+ZIo26ZdmyZbnIDcsN3DAmQELHphiSpZckJMCmUMIm2SXZbMKyybvZvLspvCEbSEgjBRwIYMDgUEwvLsHBTcZylWSr2JKtUZnRlPP+cUf2SJbssfFItu75PI8e6Za5c2Y8vmd+XVQVY4wx7uXp7wCMMcb0L0sExhjjcpYIjDHG5SwRGGOMy1kiMMYYl7NEYIwxLmeJwBhjXM4SgTHGuJwlAmPSSBz2/8wc1+wDalxBRO4Rkc0iEhSR9SLyyaRjt4rIhqRjsxL7R4nIX0SkQUT2iMhPE/vvFZHfJz2+TERURHyJ7VdF5Hsi8hbQBowTkZuTnmOLiPxjt/guF5HVItKciHO+iFwlIqu6nfdPIvJ0+t4p40a+/g7AmD6yGTgLqAWuAn4vIhOAM4F7gSuAlcB4ICIiXuBZ4BXgJiAGzDmC57sJWABsBASYBFwKbAE+DjwvIitU9W8iMhf4HXAl8DJQAuQBW4EHRWSyqm5Iuu53j+YNMKY3ViIwrqCqf1bVnaoaV9XHgE3AXOAW4AequkIdlaq6PXFsBPB1VW1V1ZCqvnkET/kbVV2nqlFVjajqc6q6OfEcrwF/xUlMAJ8HfqWqLybiq1HVClUNA48BNwKIyFSgDCdBGXPMWCIwriAin05UvewVkb3ANGAIMAqntNDdKGC7qkaP8imruj3/AhF5V0QaE89/ceL5O5+rpxgAfgtcLyKCUxpYlEgQxhwzlgjMgCciY4BfALcDRao6CFiLU2VThVMd1F0VMLqz3r+bViA7aXt4D+fsn9ZXRDKBJ4D/BoYlnn9J4vk7n6unGFDVd4EOnNLD9cAjPb9KY46eJQLjBjk4N+YGABG5GadEAPBL4GsiMjvRw2dCInEsB3YB3xeRHBEJiMgZicesBj4uIqNFpAD4xmGePwPITDx/VEQWABcmHX8YuFlEzhMRj4iUikh50vHfAT8FIkdYPWVMSiwRmAFPVdcD/wO8A9QB04G3Esf+DHwP+CMQBJ4CBqtqDLgMmADsAKqBaxKPeRGn7v4DYBWHqbNX1SBwJ7AIaML5Zr846fhy4GbgR8A+4DVgTNIlHsFJXL/HmDQQW5jGmOObiGQB9cAsVd3U3/GYgcdKBMYc/74IrLAkYNLFxhEYcxwTkW04jcpX9HMoZgCzqiFjjHE5qxoyxhiXO+GqhoYMGaJlZWX9HYYxxpxQVq1atVtVh/Z07IRLBGVlZaxcubK/wzDGmBOKiGzv7ZhVDRljjMtZIjDGGJezRGCMMS53wrUR9CQSiVBdXU0oFOrvUNIqEAgwcuRI/H5/f4dijBlABkQiqK6uJi8vj7KyMpzZegceVWXPnj1UV1czduzY/g7HGDOADIiqoVAoRFFR0YBNAgAiQlFR0YAv9Rhj+l5aE0Fi3dWNIlIpIvf0cHy0iCwTkfdF5AMRufgjPNdHC/YE4IbXaIzpe2lLBIk1Xx/AWbd1CnCdiEzpdtq3cFZcmglcC/wsXfEYY46t+mCI37y1lbpmK6We6NJZIpgLVKrqFlXtAB4FLu92jgL5ib8LgJ1pjCdt9u7dy89+duQ57OKLL2bv3r1piMiY9ApHY9z2u1Xc+8x6zvj+K9z16PusrjrxP8uqSmV9Cy+srWVtzT5CkViv571SUcedf3qfv+1o+kjPGYsrDcEwG3Y188amBrbubv1I1zsa6WwsLqXruq3VwGndzrkX+KuI3IGzitT5PV1IRG4DbgMYPXr0MQ/0o+pMBF/60pe67I9Go/h8vb/FS5YsSXdopr+0N4F4IZBPLK5EYnECGobKl2Dr6zBiJkxZCJl5PT8+HIT3/wD7qiDSDtEQxCIHjnv9UHYWTJoPWYVdHhqPKx7PYaoRox3QXA0eP/izwBcAT9Jntb0J6tZB3VrijVvxjPsElF8KvgwA/uPZ9ayu2st9l09l2+42Fq2s4pnV1Vx3Whn3XT4Nb+fzx2NQ9R5sfwtyh0HRBBg8HnKGgMfbNSZViEed15aqSDu0NxGNxfAVlIIn6bvtvhqoWeWck1UI2YOdn7wRNHV4WLaxnoZgmLaOGG0dUbbubuPv2+qZFX6PT3g+oIJBPKzDCeWPZdCw0YwcUcqkkcXsa4/w0Otb2FgXxCOwdF0t9183k4umDANVVIRnPtjFm5sauGHOcE4J1EN9hfPcw6ZBnnNerH4jb7/8FLUV77InnkOdFlKrgxkkLZyeU8uswE4KPa3sHno62wo/xoeBkzl9UinTSgtSf39SlLbZR0XkSmC+qt6S2L4JOE1Vb086558SMfyPiJyOs2TfNFWN93bdOXPmaPcpJjZs2MDkyZPT8TJScu211/L0008zadIk/H4/gUCAwsJCKioq+PDDD7niiiuoqqoiFApx1113cdtttwEHpstoaWlhwYIFnHnmmbz99tuUlpby9NNPk5WVddBz9fdrNYcQj8OWV2DVb4lXLMGjUeq9w1gTHUUMD2d7PyAjHgJvBsQ6wJcF5ZfApAUwfLpzg4yGYPlD8Pb/g/ZG8GcnbtRZ4PWxf5njcBDadjs377GfgPwSOppqqK/ZSmZHE5l+L9l+Lz6vh3jWYOqliDXBXOId7Uz1VlES2YFXoym9rBYNkCshYllFeGfewMrWoSxftYJzhzZTHtgLbY1oeyPS0UKDFrAnZyITT5mHN9KGbngWaa3v+e0SP/gDiNfvvO5IO4Ki2UXI4PFQNB68GbTuriJYv4OMjj3k+L1k+jyAQrgFou37r9chGUTzx5BVNAppqIBg7xUMjZpLrRaxQ4vZpsOp8ZRQntnIQn2FvGgjMX8enkgLQtf7Y0j91OsgajLGUThuBiXjpvPC629S2rKWuRnb8UdbaJFcGmI5xMXLGGrxS7dSRc5QYgretgYAgp58srUdrx5I9G2SzbrYSNo1k7meCgISoU0zeX/aNznjqq+k9O/WnYisUtU5PR5LYyI4HbhXVS9KbH8DQFX/M+mcdTjJoiqxvQWYp6o9f3I4fCL492fWsX5n8zF9LVNG5POdy6b2enzbtm1ceumlrF27lldffZVLLrmEtWvX7u/m2djYyODBg2lvb+fUU0/ltddeo6ioqEsimDBhAitXrmTGjBlcffXVLFy4kBtvvPGg57JEcJTiMahaDhnZUDw1cVMF6tbD334LFUucb2rDpjrf2saf69yIDiXaAbs3QvVK52fr67BvB82eAh7tOJMguZyavYtyzw580TaeC53MG/6PMfcTl/Kx7CqGb3uKgs3P4g0nqha8meDLhHAz4bLz+HDK7ayOj2dTXZCNtUFCkRjlw/OZXJLH5OG5lMcrKdi6BCqeI9wWZFMon13xQrILh1HVFCIWV0oKMpDW3QyO72GktwmP10eFjub9cClbdTgjCgLMn1RAeZEf0Rh72yNsrA2yYmeYN/YNoyF7HKeWl7F3zV/5FC9zvmcVXmJE8eEtGosUjoHsIc437kA+GzauJ7pzDeXeGjxeH8u9s/ljcAbbC09ndFYHhaEdFHVUQ1sTGXSQRZgMidOmGbTjJ6o+xvibKM9oYKTuQmMRqqKDqKeI9ozB7AvFGTU4m1PLCtne4mFJZZigJ4+pw/Noqd1IaWwXI71NbKOE9yLjWR2fwF5yKKSFAmlhiDRzUlaQOYPDTAjsJbe1Ctm7DYl1OCW4ky6C2Z+FCec7pZOmbbBnM7TWEw7upml3Hb6WnRS1ViK7N4HGUPFQ5R/HG21jaCSPob525pV4KM3zsKp1KI9V5bMpPpLpg6OUs4MJ8W3sCbaxismcdu7lLDjrdKcjSFsjNNdAIB8GjaG+JczG2iCF/jjDm5ZTUP0qvpnXISN7vJcf1qESQTqrhlYAE0VkLFCD0xh8fbdzdgDnAb8RkclAgMQC4yeyuXPndunrf//99/Pkk08CUFVVxaZNmygqKurymLFjxzJjxgwAZs+ezbZt2/os3gGtdi188Bis+TMEdzn7/NnES2YQj0bw7VzhfEOfcAGEm9F1TyGrfoMiMPky5My7oXQWNO+ETS/C5leI7t5MdG8NgY7G/U/T5i1gW/Y0fh69gje9p/HFC6fw5dPHEPAfqP6YWbOPpc9X8N0XKhN7FuDjAiZKDVO9Vcz01lAUC/Lr2Dm8VzEeKtqBteRl+pg4LJfcgI8XN9Tx2MoDNa6Dss+idNCFrNvbTPnwPO6/biYnDcujsbWD3769jZ+tqmbKuHw+ffoYCscPweMRTgemtEV4uaKO+1/exE/eaWN6aQGKsrbG+RJ1ysgCPn1BGZeeUkKmz0v9/Cn8z9KL+eaqdYzOjfHQ7Z9kaEHOQW/35HPgkXe2ccXTH+BBKcjN5mtXTOLHc0YdqC4CQpEYFbVB1lTvZUdTOxk+DwG/F78IKxtb+WNdCx/WBhmU4+f6uWO45tRR5Ad8/OKNrdzy0ofQCB3ROPPGDeaHV89gxKAsQpEYL6yt5YENdeRn+RlZmMXNg7IYlJ2BzyN4PUJBlp/y4Xlde+DFY04VnD8bcosP7Pd4Yegk5wfIBIYnv9hICJq2IYNGUerLZueLGwlH4nz63IkUZDvVW/OAMfvaefC1LVQ1tlEZmklze5QRZQHuu3waowZnH7heTpHzk1CcF6A4L+BslF0GMy/r4QN+bKQtEahqVERuB5YCXuBXqrpORO4DVqrqYuCrwC9E5G6chuPP6kcsohzqm3tfyck58B/k1Vdf5aWXXuKdd94hOzubs88+u8exAJmZmfv/9nq9tLe3H3SOARq3Ojfk0D6Y94We69jDQVjzuPNNf+f7TvXJhAvQC7/L1oYWdq1/k9wdq8nUDvJO+xaln/jc/v+AP1pawRPL3uVa3zI+v/ElsjcshoLRsG8HAHu8Q/kgUsqu+AxaA8OIDSrjnVAZK5sLaN8d59q5o/nrBScxJDfzoLCmlRbw+1tOY8OuZna3hGkNO3XTe9tOZndLmNXBMC3hKNMHZXFpUTaji3KYWJxLSUFg/41LValPNCxubmhlc0MLWxta+eLZ4/nK+RPJ9DmJZ3BOBndfcBJ3X3BSj29jQbafT80aycJTRvCX92v4xetbyMn08c2Ly5k/tYTRRdldzi/OC/BfV55M5cfHEvB7GVqQ3eN1AW46vYwhuZlsrAvy+TPHkhc4uM4/4PcyY9QgZowa1Ot1VPWgLtNfPHs8F04dxn89X8HsMYXccta4/Qkm4PdyxcxSrphZ2us1e+TxQmHZkT0GwB+A4nLAucF9/aLyHk8rKcji3oX9f186lLSOLFbVJcCSbvu+nfT3euCMdMbQF/Ly8ggGgz0e27dvH4WFhWRnZ1NRUcG7777bx9Ed59r3JurAMwlFYrR3xCjMyeh6TkcbvH2/c3Pfk7Rs79//CP/wsPONHZwk8c5PYfWfINJKfOhkts7+N17N+AQrGzysfnYvu/ZlkeW/kgXTbmfF9kaaV0T54wwfU3PghbW7uH/ZZq6aPZNIwenMfeVS/nP0Ki4p2MrKoZ/ivo2l7GA0N3ysjAXThjO9tAAR4Qs4N61ITMnwHb4j3uSS/MOe0xsRYVh+gGH5Ac6edNSX2c/n9XD1nFFcPWdUSudPKO6lcbubBdNLWDC95KOE1uu4mfFDc3no00dXPWJ6NiCmmOhvRUVFnHHGGUybNo2srCyGDRu2/9j8+fP5+c9/zuTJk5k0aRLz5s3rx0j7zv5vc/EYPPUlaK2Hj90B484BEWhpgNd/ACt/hSLsLZjMi/tGsjw6gQsvuZILTzvFudDG52HJP8O+Hei4s4nO/hzNI8+GYC2Dl96BPHwBnPU1aNwMa59AxcvusQtZxAU8uLmQ5qoYUM+YomxmjSnknEnFLJg2nJxMH1WNbVzz4Dvc9PBy7rt8Kv/8+AfMGDWI735yGpk+Lx6PcMdL2dyXdz4NwTDnlhfz8KemMyw/cNDrFREyfDbgz5yYTrg1i4/HXkN96bh6rZUvQX4pFHeNZ8OuZm56eDl3njueT+/9mdMLJmuw0wumZAaUnQmrfgORdponX8vrVRGG7FvDDM8WAoQBaMgcQ9GwkXh2vEVTzji+J7eyuGksHbEDHcqGeNv4YdbDfDz6Du2SxZOeC/lZ6EKqY4VkZ3i5aOpwLjulhNmjB++vs+1u6+5WrnnwHeqDYYrzMnnmjjO73OgfWFbJ797Zxj9fVM6nZpXa6G5zwuqXXkPpYongGL3WdU/BrtXOjTx/BBRPgcEpTmYXj8GL33aqYcQL874IZ38DMnOJxOJc8cBbrNvZzOe8z/Nt/yNw+u1w3rfh74/CWz+Bxs00ly3gfrmOX2/0k53h5WsXTuLGuaXorg94bekTyPa3KPft5PeR8/hFdAHTRg9h7tjBFGT5yU/UOVc1tbG9oZXAng9oyRpFXuFQhhcEmFySz/mTi8nOSK3AW1kf5L5nN3D3+ROZObrw8A8w5gTUX72GzPFq7V/g8Ztx+qQnvgh4fPCZZ2DMx7qeu/EFp8F1wnlQOhs6WuDxz0Pli3DqrRCPOAlh3ZPwiX9hcVUue3cGWXRWnDkrfs8L8blkjL6dc32ZhE6+kVczzuOZd9byXEWM3Ewfnz9zNLeeNY6heYnG1VGzOe+W2fx1XS1ffm0zs0cXsuTUUUwcdqi66Y9WXzyhOI/ffW7uR7qGMScyKxGcYD7ya61ZBb++2BnZetOTTg+bfVXwxC1Od7gvvHmgC9vWN+CRTzo3e0CzCmknQCDUwMvjvs7bgy5j1uhCLh60A++Sr0Ld2i5PFRsxh6tD32RtfQcXTy/hpQ11BENRhuZlcvMZZdxw2hgKsmxtBWP6gpUIjGNfDfzpemeo/zW/d3rr+LOcvtNX/QZ+eQE8+Y9w/SKnd85jNziDqq5/jNaty1nx18fIb9vG/41+jnfXTyXTt4Nfv7WNHw/N4Y5z/sTLb7yB7qvhP88vIo92vDOu56F4Dlc/+A5/XVfLRVOHc8XMUj42vgifd0DMgG7MgGCJwA1iUdjxNrzwTehohU8/5cz1kqzkFJj/f+C5r7LqV3dS3vgKWd5MPNcvopqhfPbVYewI3sL3/2E6P588jLxM56Pz/Npa7n95E19ZtAYYxP3XnUPeKSP2X7YIeO7OswC6DK4yxhw/LBEMNI1boGm7M2lYexPs+jtUPOfMS+PPgat/d1Avn046+3OsefM5Zlc/QrtmcE38O5S80Mg7WzYRjsT43efnMm9c1xHRl5xcwoJpw/nr+lpq94W47OSD+45bAjDm+GaJoB/k5ubS0tJy7C7YUg9rn3B65exa3fVYRq4zf8rkhTDxAsg4eFqATg++sZWf1l3Pn0d48M66gZMaJvLcml3kBXz84ZbTOKmXBluPR5g/7aMNHjLG9B9LBCey2rXw1o+dXkAac6p3Lvo/TkNwVmFi6t0i8PqpD4b4/pMVROLK4Gw/hTkZjB2Sw5yywZQOyuK5D3bx/ecruPTk8Uy69nE8HuF7wL8vnIrXI9Z/3pgBzBLBMXDPPfcwatQovvzlLwNw77334vP5WLZsGU1NTUQiEb773e9y+eXd1+U5QvGYM33x2ifg74/BpqXON/7TvgCzPr1/3pPuorF4YgGNvYwoCNDUFmFf+4Epb0sKAuxp7WDOmEL++6pTusxlb426xgx8Ay8RPH8P1K45ttccPh0WfL/Xw9dccw1f+cpX9ieCRYsWsXTpUu68807y8/PZvXs38+bNY+HChUf2zToedbp3hpqd3/EIBOth6eecb/rnfAtO/byz4MUh/PilTby7pZH/vuoUrpw9EoBILM7G2iArtzWycnsT+9oj/OTamVafb4wLDbxE0A9mzpxJfX09O3fupKGhgcLCQoYPH87dd9/N66+/jsfjoaamhrq6OoYPH37wBeIxZzbN9iaIhZ2VmjTuJAJwRu9m5jldPXPiTl//ISc5c9cfxrKN9fx0WSXXzBm1PwkA+L0eppUWMK20gM+ekeKIYmPMgDTwEsEhvrmn01VXXcXjjz9ObW0t11xzDX/4wx9oaGhg1apV+P1+ysrKDp5+OtIOLXVOEtC4My++PxvE40zM5vE7CSAjx9kG8DfB8AO9fiKxONv3tDF2SE6X+d7BmUfn7sdWUz48j3+//PieBtcY038GXiLoJ9dccw233noru3fv5rXXXmPRokUUFxfj9/tZtmwZ27dv7/qApu3OJGziTTTsDu56wz+MqsY2/rR8B4tWVrO7JcyQ3AzOLS/m3PJidjS28fzaWt7fsZfcTB//e+Nsq/IxxvTKEsExMnXqVILBIKWlpZSUlHDDDTdw2WWXMX3aVOZMn0T5xHGwuxJyws63//YmyCl2Rvl6U/tniMXjtIaj3PjL93hr824EOGdSMWeXF7N8ayPPr6ll0cpqAKaV5vP1iyax8JQRXVdBMsaYbiwRHENr1hxopB5SNJh3lj7hVP14/E7vHuKgcVpqKpwEkEIdPzi9fnbuC9HcHqGpLcL2xlbuOHci1546ihGDnAXub5o3ho5onPd3NFFSkHXQClPGGNMbSwTpEA07q2VF250qn4JSZ3bPo9ARjbN1dysdsTiDczKI5WXy+tfP6bH3UYbPw2ndRv4aY8zhWCI41mIR2FPp9AQqHAtZva/JejihSIytu1uJx5WxRTnkBnw07/LY4C5jzDE1YBJBTwtd97l4NJEEolA04ZDTOfQkGosTisToiMUJR+M0tnbgEWHc0FyyMrycaFOGG2NODAMiEQQCAfbs2UNRUdGxTwYdrU7PHv/B69R2EY/Bni1OtdDgcUeUBCLROA0tYRpbO4gnbvYiQpbfy+jBWWT4nCSwZ88eAoHDxGGMMUdoQCSCkSNHUl1dTUNDw7G9sMahucYZ4OULOH36fYGuXTw17owHCAedaqGcImfef2oOe/lIzOkF1NoRA4XsTC9Zfi8+r+AVDxGBzXsOnB8IBBg5cmTvFzTGmKMwIBKB3+9n7Ng0jI7d8Ay8cCPMuBE2vwzBXZA9BArHQF6JM/jrw6UQ3gdFE+Hcf4WpFx7yklWNbTzzwU6efn8nG+uC+L3ClbNH8qWzJ1g3T2NMvxgQiSBtKpZAYBBc9mNne/3TsHkZBHc6bQFte2DSfJj1GWet316qpTbVBVn89528uL6OitogALNGD+I/Lp/KxdNLKMpNrRupMcakQ1oTgYjMB34CeIFfqur3ux3/EXBOYjMbKFbVo+9mcyzFY/DhCzDxQvAm1tWdfqXzcwQ2N7Rwyf1vEo3HmVM2mG9dMpkLpwy3fv7GmONG2hKBiHiBB4ALgGpghYgsVtX1neeo6t1J598BzExXPEes6j1nCojyiz/SZb733AYyfB6W3X02pYnBX8YYczxJ52Tzc4FKVd2iqh3Ao8ChJuS/DvhTGuM5MhXPOZPATTj/qC/x2ocNvFJRzx3nTrAkYIw5bqUzEZQCVUnb1Yl9BxGRMcBY4JVejt8mIitFZOUx7xnUE1XYuATGftzpKXQUorE43312PWOKsvnsGWXHNj5jjDmGjpflp64FHlfVWE8HVfUhVZ2jqnOGDh2a/mgaNjqLwE9acNSX+OPyHWyqb+GbF08m02czfxpjjl/pTAQ1wKik7ZH03rn+Wo6naqGNS5zfk46ufaCptYMfvvghp48r4sIpw45hYMYYc+ylMxGsACaKyFgRycC52S/ufpKIlAOFwDtpjOXIbFziLACfP+KIH/rahw1c+v/eJBiK8m+XTun/aS+MMeYw0pYIVDUK3A4sBTYAi1R1nYjcJyILk069FnhUj5eJdIJ1UL3yiEsDe9s6+Oqiv/OZXy0n0+/hsdvmMWVEfpqCNMaYYyet4whUdQmwpNu+b3fbvjedMRyximcAhfJLUn5IPK7c+PB7bNgV5PZzJnD7uRNsRTBjzAnDRhZ3t+YJGFoOxVNSfsgzH+xkbU0zP7z6FD41y+YCMsacWI6XXkPHh71VsONtZ/RwinX7kVicH734IeXD87hiRo+9Y40x5rhmiSDZ2iec39NSn0bizyur2banja9dOAmPxxqGjTEnHksEydY8DqVzYHBqM5mGIjHuf3kTs0YP4rzJxWkOzhhj0sMSQaf6CqhbA9OvSvkhj7yzndrmEF+/qNy6iRpjTliWCDqtfRzEA1M/mdLpLeEoP3u1krMmDuH08bZgvDHmxGWJAJy5hdb82ZlbKC+1kcBP/q2aprYIXzn/pDQHZ4wx6WWJAKBmFTRtS7laSFV55N3tTC8tYNbo42P5BGOMOVqWCMBpJPZmwuTLUjp9+dZGPqxr4aZ5Y6xtwBhzwrNEAFD1LoyeB4GClE5/5N3t5Ad8XHbKkc9FZIwxxxtLBPEY1G+A4dNTOr0+GOKFtbVcNWcUWRk2jYQx5sRniaBxK0RDKU8p8djyKqJx5YbTRqc5MGOM6RuWCOrWOr+HTT3sqdFYnD8u38FZE4cwbmhumgMzxpi+YYmgfr0zfmDopMOe+nJFPbv2hbhx3pg+CMwYY/qGJYK6dTB4PPgPv7j8E6uqKc7L5Lxym07CGDNwWCKoW5dStVAwFOHVDxu4eHoJPq+9bcaYgcPdd7RwCzRtTSkRvLyhno5onEtPLumDwIwxpu+4OxE0VDi/U0gEz36wi+H5AWaNLkxzUMYY07fcnQjq1jm/D9N1NBiK8HqiWsjWHDDGDDSWCDJyYdChewG9tKGOjlicS6xayBgzALk7EdSvh+LJ4Dn02/DcB7sYURBg5iibYM4YM/C4NxGoOoPJDtM+sK89wusf7rZqIWPMgOXeRBCshfYmKD50InhpvVMtdLFVCxljBij3JoLOhuLDlAieW7OL0kFZVi1kjBmw0poIRGS+iGwUkUoRuaeXc64WkfUisk5E/pjOeLqo70wEvfcYisbivFW5mwumDLN1B4wxA5YvXRcWES/wAHABUA2sEJHFqro+6ZyJwDeAM1S1SUT6bu6GunWQXwpZvY8L2LanjXA0zvTS1NYpMMaYE1E6SwRzgUpV3aKqHcCjwOXdzrkVeEBVmwBUtT6N8XRVt/6w4wcqapsBKC/J61d0F5wAABTiSURBVIuIjDGmX6QzEZQCVUnb1Yl9yU4CThKRt0TkXRGZ39OFROQ2EVkpIisbGho+emSxiDOq+DDtAxW7gng9woRim3LaGDNw9XdjsQ+YCJwNXAf8QkQOapVV1YdUdY6qzhk6dOhHf9b2JohHoGDkIU+rqG1m3JAcMn22EpkxZuBKZyKoAUYlbY9M7EtWDSxW1YiqbgU+xEkM6RUNOb99gUOetmFXkPKS/LSHY4wx/SmdiWAFMFFExopIBnAtsLjbOU/hlAYQkSE4VUVb0hiTI9rh/D5EImgORajZ2075cGsfMMYMbGlLBKoaBW4HlgIbgEWquk5E7hORhYnTlgJ7RGQ9sAz4uqruSVdM++0vEWT0esrG2iAAk62h2BgzwKWt+yiAqi4BlnTb9+2kvxX4p8RP34mFnd+HKBFUJBJB+XCrGjLGDGz93VjcP6KJRODtvURQsauZ/ICPkoJDtyMYY8yJzt2J4DAlgvLh+Tai2Bgz4Lk8EfRcIojHlY21QRtIZoxxBXcmgsO0EdTsbaclHLX2AWOMK7gzEXR2H/Vm9nh4wy6bWsIY4x4uTQSd3Ud7TgSdPYYmDbNEYIwZ+FJKBCLyFxG5REQGRuLYXzXUcyLYWBtkTFE2OZlp7V1rjDHHhVRv7D8Drgc2icj3RWRSGmNKv+ihE8GG2mYbUWyMcY2UEoGqvqSqNwCzgG3ASyLytojcLCL+dAaYFvvHERycCNo7Ymzb3cokayg2xrhEylU9IlIEfBa4BXgf+AlOYngxLZGl0yFKBJvqg8QVJluJwBjjEilVgovIk8Ak4BHgMlXdlTj0mIisTFdwaRMLO6OKexgs1jnH0EmWCIwxLpFqa+j9qrqspwOqOucYxtM3ouFexxBUNrSQ4fUwZnB2HwdljDH9I9WqoSnJC8aISKGIfClNMaVfNNzrPEOb61soG5KNzzswOkgZY8zhpHq3u1VV93ZuJNYYvjU9IfWBaLjXHkOV9S22NKUxxlVSTQReSZp9TUS8QO9Tdx7vYj0nglAkxo7GNsYPtURgjHGPVNsIXsBpGH4wsf2PiX0npmiox66j2/e0EVesRGCMcZVUE8G/4Nz8v5jYfhH4ZVoi6gvRjh5LBJX1LQBWIjDGuEpKiUBV48D/Jn5OfNFQr4lAxBKBMcZdUh1HMBH4T2AKsL/fpaqOS1Nc6RXrpUTQ0ELpoCyyMrz9EJQxxvSPVBuLf41TGogC5wC/A36frqDSLhrusY3AegwZY9wo1USQpaovA6Kq21X1XuCS9IWVZj10H43FlS0NLUywaiFjjMuk2lgcTkxBvUlEbgdqgBP3jtlD99GapnbC0biVCIwxrpNqieAuIBu4E5gN3Ah8Jl1BpV0PU0xUNjhzDFkiMMa4zWFLBInBY9eo6teAFuDmtEeVbj1MMbG5vhWwHkPGGPc5bIlAVWPAmUdzcRGZLyIbRaRSRO7p4fhnRaRBRFYnfm45muc5Yj2VCOpbKMrJoDDnxB0wbYwxRyPVNoL3RWQx8GegtXOnqv6ltwckShIPABcA1cAKEVmsquu7nfqYqt5+ZGF/RLEw+Lre8CsbWhhv1ULGGBdKNREEgD3AuUn7FOg1EQBzgUpV3QIgIo8ClwPdE0HfUk0MKAsk7VIq61u45OSSfgzMGGP6R6oji4+mXaAUqErargZO6+G8fxCRjwMfAneralX3E0TkNuA2gNGjRx9FKEliEed3UhvB7pYO9rVHrOuoMcaVUh1Z/GucEkAXqvq5j/j8zwB/UtWwiPwj8Fu6ljo6n+ch4CGAOXPmHBTHEYmGnN9JJYLOOYasx5Axxo1SrRp6NunvAPBJYOdhHlMDjEraHpnYt5+q7kna/CXwgxTjOXqxDud30jiCygZLBMYY90q1auiJ5G0R+RPw5mEetgKYKCJjcRLAtcD13a5TkrT+8UJgQyrxfCSdJYKkqqHN9S3kZHgpKeh5+UpjjBnIUi0RdDcRKD7UCaoaTYxCXgp4gV+p6joRuQ9YqaqLgTtFZCHOHEaNwGePMp7URcPO76Sqoa27WykbkoP0sJi9McYMdKm2EQTp2kZQi7NGwSGp6hJgSbd93076+xvAN1KK9FjZnwgOlAjqmkOMLLTF6o0x7pRq1VBeugPpM7GDSwT1wTCzxxT2U0DGGNO/UpprSEQ+KSIFSduDROSK9IWVRtFEY3GijaAjGqextYPiPGsfMMa4U6qTzn1HVfd1bqjqXuA76Qkpzbp1H21ocUoIw/IPXp/AGGPcINVE0NN5R9vQ3L+6dR+ta3YSw7B8KxEYY9wp1USwUkR+KCLjEz8/BFalM7C02V8icBJBfSIRFFuJwBjjUqkmgjuADuAx4FEgBHw5XUGlVWevIW9niaCzashKBMYYd0q111ArcNA00iek/d1HD1QN+TzC4GybftoY406p9hp6UUQGJW0XisjS9IWVRrHuiSBMcV4mHo8NJjPGuFOqVUNDEj2FAFDVJg4zsvi41a1EUB8MUWzVQsYYF0s1EcRFZP/8zyJSRg+zkZ4QDmojCFnXUWOMq6XaBfRfgTdF5DVAgLNIrA9wwjmojSDMvHFF/RiQMcb0r1Qbi18QkTk4N//3gaeA9nQGljaxMHh84PESisTY1x6xHkPGGFdLddK5W4C7cNYUWA3MA96hh0VkjntJC9fXJ7qOFudZ1ZAxxr1SbSO4CzgV2K6q5wAzgb2HfshxKhreP89QXdBGFRtjTKqJIKSqIQARyVTVCmBS+sJKo2jIppcwxpgkqTYWVyfGETwFvCgiTcD29IWVRrGOLg3FYBPOGWPcLdXG4k8m/rxXRJYBBcALaYsqnaKh/V1H65tDZPg8FGT5+zkoY4zpP0c8g6iqvpaOQPpMtKNL1dCw/ExbotIY42qpthEMHLFwl6qhYbYgjTHG5dyXCJK6j9YFQ9ZQbIxxPXcmgkT30frmsK1DYIxxPXcmAl+AlnCUlnDUSgTGGNdzXyKIhcGXcWBlMhtVbIxxubQmAhGZLyIbRaRSRHpd2EZE/kFENDGfUXpFQ+AL2MpkxhiTkLZEICJe4AFgATAFuE5EpvRwXh7OFBbvpSuWLqId4M2gfv/0ElYiMMa4WzpLBHOBSlXdoqodOGsdX97Def8B/BfOOsjplygR7J9wzkoExhiXS2ciKAWqkrarE/v2E5FZwChVfS6NcXQV6wBfBnXNIbL8XvIyj3hMnTHGDCj91lgsIh7gh8BXUzj3NhFZKSIrGxoaPtoTd7YRBMM2qtgYY0hvIqgBRiVtj0zs65QHTANeFZFtOGscLO6pwVhVH1LVOao6Z+jQoUcfUSwKGgdvJnXNtlaxMcZAehPBCmCiiIwVkQzgWmBx50FV3aeqQ1S1TFXLgHeBhaq6Mm0RRRPNEL5M6pttVLExxkAaE4GqRoHbgaXABmCRqq4TkftEZGG6nveQYh1ObN6MxDxD1mPIGGPS2lKqqkuAJd32fbuXc89OZyzA/hJBCB/tkZhNL2GMMbhtZHHU6TLaFnPy36DsjP6MxhhjjgvuSgSJqqH2uJMIrOuoMca4LREkqoba1UkAuQFLBMYY47JE4JQIWmNeAPICtkSlMca4LBE4JYKWaKJEYFVDxhjjskQQcxqLD5QILBEYY4y7EkGi11Bz1BKBMcZ0cmUiaIl48HqELL+3nwMyxpj+566vxIlEsC/iITdTbMI5Y4zBbYkglpwIrDRgjDHg0qqhvR0eax8wxpgEVyaCprAlAmOM6eTORNDhsTEExhiT4K5EEAuDeGjuUBtVbIwxCe5KBIllKoOhiM0zZIwxCS5LBB3gzSAYitrMo8YYk+CuRBALo74A4WjcGouNMSbBXYkgGibudRajscZiY4xxuC8RiNNInGuNxcYYA7gwEUQ9zjrFVjVkjDEOdyWCWJiYxykJWGOxMcY43JUIomEi4rQR2DgCY4xxuC4RdNDZRmAlAmOMAdclgtCBRGBVQ8YYA6Q5EYjIfBHZKCKVInJPD8e/ICJrRGS1iLwpIlPSGQ+xDsLqJABrLDbGGEfaEoGIeIEHgAXAFOC6Hm70f1TV6ao6A/gB8MN0xQNANEQIP36vkOlzV2HIGGN6k8674VygUlW3qGoH8ChwefIJqtqctJkDaBrjgWgHobiP3EyfrU5mjDEJ6awfKQWqkrargdO6nyQiXwb+CcgAzu3pQiJyG3AbwOjRo48+omiIdq/PegwZY0ySfq8fUdUHVHU88C/At3o55yFVnaOqc4YOHXr0TxbroD1RIjDGGONIZyKoAUYlbY9M7OvNo8AVaYwHoiFaYj5rKDbGmCTpTAQrgIkiMlZEMoBrgcXJJ4jIxKTNS4BNaYsmHod4lNaY1xKBMcYkSdsdUVWjInI7sBTwAr9S1XUich+wUlUXA7eLyPlABGgCPpOueIg5y1S2xLxWNWSMMUnSekdU1SXAkm77vp30913pfP4uoiEAglGPNRYbY0ySfm8s7jPRDgCCUa9NL2GMMUlclAicEoH1GjLGmK7ckwhiTomgQ/3kW4nAGGP2c08iSJQIwvitasgYY5K4KBE4JYIwPvIyrbHYGGM6uSgRdJYIMqxEYIwxSdyTCBLjCDrUGouNMSaZexJB1EkEYfzk2zgCY4zZz3WJoMMai40xpgvXJYIwfqsaMsaYJO5JBIk2ArwZZNjqZMYYs5977oiJEoE/M6ufAzHGmOOL6xJBZiC7nwMxxpjji3sSwaDRrM6aR4aVCIwxpgv3tJpOWcj3Xh9CwOOe3GeMMalw1V0xGIra6mTGGNON6xKBjSEwxpiuXJUIWsJRG1VsjDHduCYRqCot4agNJjPGmG5ckwjaIzFicbWqIWOM6cY1iaAlFAWwxmJjjOnGNYmgOZEIrGrIGGO6ck0iaAlbicAYY3rinkSwv2rIeg0ZY0yytCYCEZkvIhtFpFJE7unh+D+JyHoR+UBEXhaRMemKJRiKAFY1ZIwx3aUtEYiIF3gAWABMAa4TkSndTnsfmKOqJwOPAz9IVzxBqxoyxpgepbNEMBeoVNUtqtoBPApcnnyCqi5T1bbE5rvAyHQFE+ysGsq0qiFjjEmWzkRQClQlbVcn9vXm88DzPR0QkdtEZKWIrGxoaDiqYEYVZnHR1GHkZHqP6vHGGDNQHRf1JCJyIzAH+ERPx1X1IeAhgDlz5ujRPMeFU4dz4dThRx2jMcYMVOlMBDXAqKTtkYl9XYjI+cC/Ap9Q1XAa4zHGGNODdFYNrQAmishYEckArgUWJ58gIjOBB4GFqlqfxliMMcb0Im2JQFWjwO3AUmADsEhV14nIfSKyMHHa/wVygT+LyGoRWdzL5YwxxqRJWtsIVHUJsKTbvm8n/X1+Op/fGGPM4blmZLExxpieWSIwxhiXs0RgjDEuZ4nAGGNcTlSPanxWvxGRBmD7UT58CLD7GIZzorP3oyt7Pw6w96KrgfB+jFHVoT0dOOESwUchIitVdU5/x3G8sPejK3s/DrD3oquB/n5Y1ZAxxricJQJjjHE5tyWCh/o7gOOMvR9d2ftxgL0XXQ3o98NVbQTGGGMO5rYSgTHGmG4sERhjjMu5JhGIyHwR2SgilSJyT3/H05dEZJSILBOR9SKyTkTuSuwfLCIvisimxO/C/o61L4mIV0TeF5FnE9tjReS9xGfkscT06a4gIoNE5HERqRCRDSJyuls/HyJyd+L/yVoR+ZOIBAb6Z8MViUBEvMADwAJgCnCdiEzp36j6VBT4qqpOAeYBX068/nuAl1V1IvByYttN7sKZIr3TfwE/UtUJQBPO8qlu8RPgBVUtB07BeV9c9/kQkVLgTmCOqk4DvDhrqQzoz4YrEgEwF6hU1S2q2gE8ClzezzH1GVXdpap/S/wdxPlPXorzHvw2cdpvgSv6J8K+JyIjgUuAXya2BTgXeDxximveDxEpAD4OPAygqh2quhf3fj58QJaI+IBsYBcD/LPhlkRQClQlbVcn9rmOiJQBM4H3gGGquitxqBYY1k9h9YcfA/8MxBPbRcDexIJK4K7PyFigAfh1oqrslyKSgws/H6paA/w3sAMnAewDVjHAPxtuSQQGEJFc4AngK6ranHxMnX7EruhLLCKXAvWquqq/YzlO+IBZwP+q6kyglW7VQG75fCTaQS7HSY4jgBxgfr8G1QfckghqgFFJ2yMT+1xDRPw4SeAPqvqXxO46ESlJHC8B3LJu9BnAQhHZhlNNeC5OHfmgRHUAuOszUg1Uq+p7ie3HcRKDGz8f5wNbVbVBVSPAX3A+LwP6s+GWRLACmJho+c/AafxxzfrIifrvh4ENqvrDpEOLgc8k/v4M8HRfx9YfVPUbqjpSVctwPguvqOoNwDLgysRpbno/aoEqEZmU2HUesB53fj52APNEJDvx/6bzvRjQnw3XjCwWkYtx6oW9wK9U9Xv9HFKfEZEzgTeANRyoE/8mTjvBImA0ztTeV6tqY78E2U9E5Gzga6p6qYiMwykhDAbeB25U1XB/xtdXRGQGTsN5BrAFuBnni6LrPh8i8u/ANTi97d4HbsFpExiwnw3XJAJjjDE9c0vVkDHGmF5YIjDGGJezRGCMMS5nicAYY1zOEoExxricJQJj+pCInN0526kxxwtLBMYY43KWCIzpgYjcKCLLRWS1iDyYWLugRUR+lJir/mURGZo4d4aIvCsiH4jIk53z9ovIBBF5SUT+LiJ/E5HxicvnJs39/4fECFZj+o0lAmO6EZHJOCNLz1DVGUAMuAFnArKVqjoVeA34TuIhvwP+RVVPxhm93bn/D8ADqnoK8DGc2SzBmf31KzhrY4zDmcvGmH7jO/wpxrjOecBsYEXiy3oWzoRrceCxxDm/B/6SmMt/kKq+ltj/W+DPIpIHlKrqkwCqGgJIXG+5qlYntlcDZcCb6X9ZxvTMEoExBxPgt6r6jS47Rf6t23lHOz9L8hw1Mez/oelnVjVkzMFeBq4UkWLYv7bzGJz/L50zUF4PvKmq+4AmETkrsf8m4LXESnDVInJF4hqZIpLdp6/CmBTZNxFjulHV9SLyLeCvIuIBIsCXcRZsmZs4Vo/TjgDOtMQ/T9zoO2fuBCcpPCgi9yWucVUfvgxjUmazjxqTIhFpUdXc/o7DmGPNqoaMMcblrERgjDEuZyUCY4xxOUsExhjjcpYIjDHG5SwRGGOMy1kiMMYYl/v/LtMH8AwvNpAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6pg2k6xO5p8",
        "colab_type": "text"
      },
      "source": [
        "### **Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT1Pr5RmWJkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "85e36f6a-1f56-4eaa-e108-cef9e49690ca"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1cWi7orKdUr",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ea-ST316j3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "25dcadad-ea0a-4d79-cad8-0b68d5127889"
      },
      "source": [
        "test_data.reset()\n",
        "predictions = model.predict_generator(test_data, steps=test_data.samples/test_data.batch_size,verbose=1)\n",
        "y_pred= np.argmax(predictions, axis=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-99bdc2a40673>:2: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "1002/1002 [==============================] - 13s 13ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJfzBH2gcQ5M",
        "colab_type": "text"
      },
      "source": [
        "### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDJVF1dbWQlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f37a28eb-de04-4762-f9ac-a7e866673921"
      },
      "source": [
        "Y_pred = predictions\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(name_as_indexes_test, y_pred))\n",
        "print('Classification Report')\n",
        "classes_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "print(classification_report(name_as_indexes_test, y_pred, target_names=classes_names))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[ 10   5  15   0   5   6   0]\n",
            " [  0  27   5   0   8   7   0]\n",
            " [  1   5  87   0   8  24   0]\n",
            " [  0   2   2   3   0   8   0]\n",
            " [  1   3  23   0  45  38   0]\n",
            " [  0   1  33   0  21 594   0]\n",
            " [  0   0   0   0   0   1  14]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec       0.83      0.24      0.38        41\n",
            "         bcc       0.63      0.57      0.60        47\n",
            "         bkl       0.53      0.70      0.60       125\n",
            "          df       1.00      0.20      0.33        15\n",
            "         mel       0.52      0.41      0.46       110\n",
            "          nv       0.88      0.92      0.90       649\n",
            "        vasc       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           0.78      1002\n",
            "   macro avg       0.77      0.57      0.60      1002\n",
            "weighted avg       0.78      0.78      0.77      1002\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMxOe1qPcUSr",
        "colab_type": "text"
      },
      "source": [
        "### **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWSBDzE5WXgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "26f2a08e-8064-4dab-89d6-161e41c1024d"
      },
      "source": [
        "cm = (confusion_matrix(name_as_indexes_test, y_pred))\n",
        "\n",
        "plot_confusion_matrix(cm, classes_names)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[ 10   5  15   0   5   6   0]\n",
            " [  0  27   5   0   8   7   0]\n",
            " [  1   5  87   0   8  24   0]\n",
            " [  0   2   2   3   0   8   0]\n",
            " [  1   3  23   0  45  38   0]\n",
            " [  0   1  33   0  21 594   0]\n",
            " [  0   0   0   0   0   1  14]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXcAwdJ-1tX",
        "colab_type": "text"
      },
      "source": [
        "### **Sensitivity & Specificity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH-wStPm6T7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = sum(sum(cm))\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc7FeOe56a9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5fa5d9c7-fec2-4ad3-c8f7-0c3010cdf348"
      },
      "source": [
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sensitivity: 0.6667\n",
            "specificity: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrPLbkZdva9",
        "colab_type": "text"
      },
      "source": [
        "# **Grad-CAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_OVC2F8xfR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_conv2d = 'conv5_block16_2_conv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyWQAabav_ST",
        "colab_type": "text"
      },
      "source": [
        "### **AKIEC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5JjxhIrqN5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "akiec_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0026492.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGMmNOxxiHxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "17c0eaab-4374-4025-80c5-00d2c2f1e3d9"
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, akiec_img, layer_name=last_conv2d)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model prediction:\n",
            "\tmel            \t(4)\twith probability 0.845\n",
            "\tbcc            \t(1)\twith probability 0.069\n",
            "\tbkl            \t(2)\twith probability 0.031\n",
            "\takiec          \t(0)\twith probability 0.026\n",
            "\tvasc           \t(6)\twith probability 0.014\n",
            "Explanation for 'mel'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c01fefc2e1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguided_gradcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0makiec_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_conv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ham10000_utils_functions.py\u001b[0m in \u001b[0;36mcompute_saliency\u001b[0;34m(model, guided_model, img_path, layer_name, cls, visualize, save)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_decode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Explanation for '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mgradcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguided_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguided_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mguided_gradcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradcam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ham10000_utils_functions.py\u001b[0m in \u001b[0;36mgrad_cam\u001b[0;34m(input_model, image, cls, layer_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0my_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mconv_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Normalize if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# grads = normalize(grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients_v2\u001b[0;34m(ys, xs, grad_ys, name, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    303\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;34m\"\"\"Implementation of gradients().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n\u001b[0m\u001b[1;32m    492\u001b[0m                        \"is enabled. Use tf.GradientTape instead.\")\n\u001b[1;32m    493\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msrc_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D53Hg3-mwNj-",
        "colab_type": "text"
      },
      "source": [
        "### **BCC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-APRYghPa4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bcc_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024332.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IavEoR0qwWO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, bcc_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnzfOod3xC7p",
        "colab_type": "text"
      },
      "source": [
        "### **BKL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Za1Ztu0kxVpc",
        "colab": {}
      },
      "source": [
        "bkl_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0025548.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xc_tDIcsxVpg",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, bkl_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2qr1BF9xEnD",
        "colab_type": "text"
      },
      "source": [
        "### **DF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w3400LGvxWEF",
        "colab": {}
      },
      "source": [
        "df_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0033626.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S92MquPFxWEI",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, df_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y6JOyRWxJy7",
        "colab_type": "text"
      },
      "source": [
        "### **MEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mRW-epcxWdq",
        "colab": {}
      },
      "source": [
        "mel_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024516.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g4j2KO_pxWdv",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, mel_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKEm-_WKxHRq",
        "colab_type": "text"
      },
      "source": [
        "### **NV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opRTYW5SxXgR",
        "colab": {}
      },
      "source": [
        "nv_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0024349.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iVbqIHvGxXgV",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, nv_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVFPRGL0xHtK",
        "colab_type": "text"
      },
      "source": [
        "### **VASC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YuzSPvQ9xXJg",
        "colab": {}
      },
      "source": [
        "vasc_img = '/content/ham10000-with-one-image-folder/HAM1000_images/ISIC_0025452.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ol1Ik8qDxXJj",
        "colab": {}
      },
      "source": [
        "gradcam, gb, guided_gradcam = compute_saliency(model, model, vasc_img, layer_name=last_conv2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYJYt3yOB47l",
        "colab_type": "text"
      },
      "source": [
        "# **Download Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFv5jXItB-fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuVkAwOn3A9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/Focal-Loss_ResNet50_model.h5')\n",
        "files.download('/content/Focal-Loss_ResNet50_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}